var documenterSearchIndex = {"docs":
[{"location":"models/linear-gaussian/#Linear-Gaussian-Models","page":"Linear Gaussian Models","title":"Linear Gaussian Models","text":"GeneralisedFilters is designed to work with the most general form of linear Gaussian state models. This can be written as:\n\n$\n\n\\begin{align} x0 &\\sim \\mathcal{N}(\\mu0, \\Sigma0) \\\nxt &= At x{t-1} + bt + wt & wt &\\sim \\mathcal{N}(0, Qt) \\\nyt &= Ht xt + ct + vt & vt &\\sim \\mathcal{N}(0, R_t) \\end{align} $\n\nwhere the model parameters A_t b_t Q_t H_t c_t R_t can be constant, time-varying, or even functions of exogenous inputs. For example, we could have b_t = B u_t for some input u_t and matrix B.\n\nMost of the time, you will not need this generality, but it is still there in case you do!\n\nIn GeneralisedFilters.jl, a linear Gaussian state space model is just a type that can return these model parameters given a time index (and possibly an input).","category":"section"},{"location":"models/linear-gaussian/#Homogeneous-(Non-Time-Varying)-Linear-Gaussian-Models","page":"Linear Gaussian Models","title":"Homogeneous (Non-Time Varying) Linear Gaussian Models","text":"If you wish to define a simple linear Gaussian model with constant parameters, you can use the concrete types:\n\nHomogeneousGaussianPrior\nHomogeneousLinearGaussianDynamics\nHomogeneousLinearGaussianObservations \n\nThese accept AbstractMatrix and AbstractVector values for the model parameters, which will be applied at each timestep. For example, we might have,\n\nμ0 = [1.0 2.0]\nΣ0 = [1.0 0.0; 0.0 1.0]\nmy_prior = HomogeneousGaussianPrior(μ0, Σ0)\n\nImportantly, the HomogeneousXYZ types are parameterised on the types of each of the fields. For example HomogeneousLinearGaussianLatentDynamics is defined as:\n\nstruct HomogeneousLinearGaussianLatentDynamics{\n    AT<:AbstractMatrix,bT<:AbstractVector,QT<:AbstractMatrix\n} <: LinearGaussianLatentDynamics\n    A::AT\n    b::bT\n    Q::QT\nend\n\nThis allows you to use specialised types for the model parameters that may capture additional structure to allow for more efficient computations. You can also take advantage of StaticArrays.jl for lightning-fast filtering in low-dimensional settings. As an illustrative example you could have:\n\nusing StaticArrays\nusing SparseArrays\nusing PDMats\n\nA = @SMatrix [1.0 0.1; 0.0 0.7]\nb = sparse([1.0, 0.0])\nQ = PDiagMat([0.1, 0.5])\n\nmy_dynamics = HomogeneousLinearGaussianLatentDynamics(A, b, Q)\n\nGeneralisedFilters.jl's linear Gaussian filtering methods have been written in such a way to preserve StaticArrays.\n\nIf your application is particularly performance-sensitive and your model does not make use of b_t and/or c_t, you can replace these with lazy representation of a zero vector from FillArrays.jl:\n\nusing FillArrays\nHomogeneousLinearGaussianObservations(H, Zeros(dim_y), R)","category":"section"},{"location":"models/linear-gaussian/#General-Linear-Gaussian-Models","page":"Linear Gaussian Models","title":"General Linear Gaussian Models","text":"GeneralisedFilters.jl defines linear Gaussian models in a fairly abstract fashion. Although this might seem a bit overkill at first glance, it leads to some powerful benefits in terms of modularity and extensibility. \n\nWe define the components of a linear Gaussian state space model as subtypes of the following three abstract types:\n\nGaussianPrior  (itself a subtype of StatePrior)\nLinearGaussianLatentDynamics (a subtype of LatentDynamics)\nLinearGaussianObservationProcess (a subtype of ObservationProcess)\n\nThese must define calc_ functions that return the model parameters for a given time index. For example, a general LinearGaussianLatentDynamics must define\n\ncalc_A(dyn::MyDynamicsType, step::Integer; kwargs...)\n\nThe calc_ functions for the prior are calc_μ0 and calc_Σ0 and these do not take a time index argument.\n\nThe kwargs... argument is included to allow for exogenous inputs to be passed to the model. For example, in the case where b_t = B u_t, we could define\n\nfunction calc_b(dyn::MyDynamicsType, step::Integer; u)\n    return dyn.B * u[step]\nend\n\nwhere u is a vector of inputs passed to the filtering/smoothing function.\n\nFinally, in cases where it is more efficient/convenient to compute the various model parameters at the same time (e.g. A_t and b_t depend on a common value that is expensive to compute), you can write a method for the following functions which return a tuple of the relevant parameters:\n\ncalc_initial(prior::GaussianPrior; kwargs...) returns (μ0, Σ0)\ncalc_dynamics(dyn::LinearGaussianLatentDynamics, step::Integer; kwargs...) returns (A, b, Q)\ncalc_observations(obs::LinearGaussianObservationProcess, step::Integer; kwargs...) returns (H, c, R)","category":"section"},{"location":"examples/trend-inflation/#Trend-Inflation","page":"Trend Inflation","title":"Trend Inflation","text":"(Image: Open in Colab) (Image: View Source)\n\nThis example is a replication of the univariate state space model suggested by (Stock & Watson, 2016) using GeneralisedFilters to define a heirarchical model for use in Rao-Blackwellised particle filtering.\n\nusing Pkg\n\nPkg.activate(\".\")\nif isfile(\"Project.toml\")\n    Pkg.instantiate()\nelse\n    Pkg.add([\n        \"SSMProblems\",\n        \"GeneralisedFilters\",\n        \"CSV\",\n        \"CairoMakie\",\n        \"DataFrames\",\n        \"Distributions\",\n        \"LogExpFunctions\",\n        \"PDMats\",\n        \"StatsBase\",\n    ])\nend;\n\n\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m \n\nproject at `~/work/SSMProblems.jl/SSMProblems.jl/GeneralisedFilters/examples/trend-inflation`\n\nusing GeneralisedFilters\nusing SSMProblems\nusing Distributions\nusing Random\nusing StatsBase\nusing LinearAlgebra\nusing PDMats\n\nconst GF = GeneralisedFilters\n\nGeneralisedFilters\n\nusing Downloads\n\nconst UTILITIES_URL = \"https://raw.githubusercontent.com/TuringLang/SSMProblems.jl/main/GeneralisedFilters/examples/trend-inflation/utilities.jl\"\nconst UTILITIES_PATH = isfile(\"utilities.jl\") ? \"utilities.jl\" : Downloads.download(UTILITIES_URL)\ninclude(UTILITIES_PATH);","category":"section"},{"location":"examples/trend-inflation/#Model-Definition","page":"Trend Inflation","title":"Model Definition","text":"We begin by defining the local level trend model, a linear Gaussian model with a weakly stationary random walk component. The dynamics of which are as follows:\n\n$\n\n\\begin{aligned}     y{t} &= x{t} + \\eta{t} \\\n    x{t+1} &= x{t} + \\varepsilon{t} \\end{aligned} $\n\nHowever, this model is not enough to capture trend dynamics when faced with structural breaks. (Stock & Watson, 2007) suggest adding a stochastic volatiltiy component, defined like so:\n\n$\n\n\\begin{aligned}     \\log \\sigma{\\eta, t+1} = \\log \\sigma{\\eta, t} + \\nu{\\eta, t} \\\n    \\log \\sigma{\\varepsilon, t+1} = \\log \\sigma{\\varepsilon, t} + \\nu{\\varepsilon, t} \\end{aligned} $\n\nwhere nu_zt sim N(0 gamma) for z in  varepsilon eta .\n\nUsing GeneralisedFilters, we can construct a heirarchical version of this model such that the local level trend component is conditionally linear Gaussian on the volatility draws.","category":"section"},{"location":"examples/trend-inflation/#Stochastic-Volatility-Process","page":"Trend Inflation","title":"Stochastic Volatility Process","text":"We begin by defining the non-linear dynamics, which aren't conditioned contemporaneous states. Since these processes are traditionally non-linear/non-Gaussian we use the SSMProblems interface to define the stochastic volatility components.\n\nstruct StochasticVolatilityPrior{T<:Real} <: StatePrior end\n\nfunction SSMProblems.distribution(prior::StochasticVolatilityPrior{T}; kwargs...) where {T}\n    return product_distribution(Normal(zero(T), T(1)), Normal(zero(T), T(1)))\nend\n\nFor the dynamics, instead of using the SSMProblems.distribution utility, we only define the simulate method, which is sufficient for the RBPF.\n\nstruct StochasticVolatility{ΓT<:AbstractVector} <: LatentDynamics\n    γ::ΓT\nend\n\nfunction SSMProblems.simulate(\n    rng::AbstractRNG,\n    proc::StochasticVolatility,\n    step::Integer,\n    state::AbstractVector{T};\n    kwargs...,\n) where {T<:Real}\n    new_state = deepcopy(state)\n    new_state[1:2] += proc.γ .* randn(rng, T, 2)\n    return new_state\nend","category":"section"},{"location":"examples/trend-inflation/#Local-Level-Trend-Process","page":"Trend Inflation","title":"Local Level Trend Process","text":"For the conditionally linear and Gaussian components, we subtype the model and provide a keyword argument as the conditional element. In this case A and b remain constant, but Q is conditional on the log variance, stored in new_outer (the nomenclature chosen for heirarchical modeling).\n\nstruct LocalLevelTrend <: LinearGaussianLatentDynamics end\n\nGF.calc_A(::LocalLevelTrend, ::Integer; kwargs...) = [1;;]\nGF.calc_b(::LocalLevelTrend, ::Integer; kwargs...) = [0;]\nfunction GF.calc_Q(::LocalLevelTrend, ::Integer; new_outer, kwargs...)\n    return PDMat([exp(new_outer[1]);;])\nend\n\nSimilarly, we define the observation process conditional on a separate log variance.\n\nstruct SimpleObservation <: LinearGaussianObservationProcess end\n\nGF.calc_H(::SimpleObservation, ::Integer; kwargs...) = [1;;]\nGF.calc_c(::SimpleObservation, ::Integer; kwargs...) = [0;]\nfunction GF.calc_R(::SimpleObservation, ::Integer; new_outer, kwargs...)\n    return PDMat([exp(new_outer[2]);;])\nend","category":"section"},{"location":"examples/trend-inflation/#Unobserved-Components-with-Stochastic-Volatility","page":"Trend Inflation","title":"Unobserved Components with Stochastic Volatility","text":"The state space model suggested by (Stock & Watson, 2007) can be constructed with the following method:\n\nfunction UCSV(γ::T) where {T<:Real}\n    stoch_vol_prior = StochasticVolatilityPrior{T}()\n    stoch_vol_process = StochasticVolatility(fill(γ, 2))\n\n    local_level_model = StateSpaceModel(\n        GF.HomogeneousGaussianPrior(zeros(T, 1), PDMat([100.0;;])),\n        LocalLevelTrend(),\n        SimpleObservation(),\n    )\n\n    return HierarchicalSSM(stoch_vol_prior, stoch_vol_process, local_level_model)\nend;\n\nFor plotting, we can extract the ancestry of the Rao Blackwellised particles using the callback system. For our inflation data, this reduces to the following:\n\nrng = MersenneTwister(1234);\nsparse_ancestry = GF.AncestorCallback(nothing);\nstates, ll = GF.filter(\n    rng,\n    UCSV(0.2),\n    RBPF(BF(2^12), KalmanFilter()),\n    [[pce] for pce in fred_data.value];\n    callback=sparse_ancestry,\n);\n\nThe sparse_ancestry object stores a sparse ancestry tree which we can use to approximate the smoothed series without an additional backwards pass. We can convert this data structure to a human readable array by using GeneralisedFilters.get_ancestry and then take the mean path by passing a custom function.\n\ntrends, volatilities = mean_path(GF.get_ancestry(sparse_ancestry.tree), states);\nplot_ucsv(trends[1, :], eachrow(volatilities), fred_data)\n\n(Image: )","category":"section"},{"location":"examples/trend-inflation/#Outlier-Adjustments","page":"Trend Inflation","title":"Outlier Adjustments","text":"For additional robustness, (Stock & Watson, 2016) account for one-time measurement shocks and suggest an alteration in the observation equation, where\n\n$\n\n\\eta{t} \\sim N(0, s{t} \\cdot \\sigma{\\eta, t}^2) \\quad \\quad s{t} \\sim \\begin{cases} U(0,2) & \\text{ with probability } p \\\n\\delta(1) & \\text{ with probability } 1 - p \\end{cases} $\n\nThe prior is the same as before, but with additional state which we can assume will always be 1; using the Distributions interface this is just Dirac(1)\n\nstruct OutlierAdjustedVolatilityPrior{T<:Real} <: StatePrior end\n\nfunction SSMProblems.distribution(\n    prior::OutlierAdjustedVolatilityPrior{T}; kwargs...\n) where {T}\n    return product_distribution(Normal(zero(T), T(1)), Normal(zero(T), T(1)), Dirac(one(T)))\nend\n\nIn terms of the model definition, we can construct a separate LatentDynamics which contains the same volatility process as before, but with the respective draw in the third component.\n\nstruct OutlierAdjustedVolatility{ΓT} <: LatentDynamics\n    volatility::StochasticVolatility{ΓT}\n    switch_dist::Bernoulli\n    outlier_dist::Uniform\nend\n\nThe simulation then calls the volatility process, and computes the outlier term in the third state\n\nfunction SSMProblems.simulate(\n    rng::AbstractRNG,\n    proc::OutlierAdjustedVolatility,\n    step::Integer,\n    state::AbstractVector{T};\n    kwargs...,\n) where {T<:Real}\n    new_state = SSMProblems.simulate(rng, proc.volatility, step, state; kwargs...)\n    new_state[3] = rand(rng, proc.switch_dist) ? rand(rng, proc.outlier_dist) : one(T)\n    return new_state\nend\n\nFor the observation process, we define a new object where R is dependent on both the measurement volatility as well as this outlier adjustment coefficient.\n\nstruct OutlierAdjustedObservation <: LinearGaussianObservationProcess end\n\nGF.calc_H(::OutlierAdjustedObservation, ::Integer; kwargs...) = [1;;]\nGF.calc_c(::OutlierAdjustedObservation, ::Integer; kwargs...) = [0;]\nfunction GF.calc_R(::OutlierAdjustedObservation, ::Integer; new_outer, kwargs...)\n    return PDMat([new_outer[3] * exp(new_outer[2]);;])\nend","category":"section"},{"location":"examples/trend-inflation/#Outlier-Adjusted-UCSV","page":"Trend Inflation","title":"Outlier Adjusted UCSV","text":"The state space model suggested by (Stock & Watson, 2007) can be constructed with the following method:\n\nfunction UCSVO(γ::T, prob::T) where {T<:Real}\n    stoch_vol_prior = OutlierAdjustedVolatilityPrior{T}()\n    stoch_vol_process = OutlierAdjustedVolatility(\n        StochasticVolatility(fill(γ, 2)), Bernoulli(prob), Uniform{T}(2, 10)\n    )\n\n    local_level_model = StateSpaceModel(\n        GF.HomogeneousGaussianPrior(zeros(T, 1), PDMat([100.0;;])),\n        LocalLevelTrend(),\n        OutlierAdjustedObservation(),\n    )\n\n    return HierarchicalSSM(stoch_vol_prior, stoch_vol_process, local_level_model)\nend;\n\nWe then repeat the same experiment, this time with an outlier probability of p = 005\n\nrng = MersenneTwister(1234);\nsparse_ancestry = GF.AncestorCallback(nothing)\nstates, ll = GF.filter(\n    rng,\n    UCSVO(0.2, 0.05),\n    RBPF(BF(2^12), KalmanFilter()),\n    [[pce] for pce in fred_data.value];\n    callback=sparse_ancestry,\n);\n\nthis process is identical to the last, except with an additional volatilities state which captures the outlier distance. We omit this feature in the plots, but the impact is clear when comparing the maximum transitory noise around the GFC.\n\ntrends, volatilities = mean_path(GF.get_ancestry(sparse_ancestry.tree), states);\np = plot_ucsv(trends[1, :], eachrow(volatilities), fred_data)\n\n(Image: )","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"Executable examples for GeneralisedFilters with links to notebooks and source files.\n\nExample Preview\nTrend Inflation<br>This example is a replication of the univariate state space model suggested by (Stock &<br>Colab · Notebook (Image: Trend Inflation)","category":"section"},{"location":"#GeneralisedFilters","page":"Home","title":"GeneralisedFilters","text":"","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"In the julia REPL:\n\n] add GeneralisedFilters","category":"section"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"GeneralisedFilters provides implementations of various filtering and smoothing algorithms for state-space models (SSMs). The goal of the package is to provide a modular and extensible framework for implementing advanced algorithms including Rao-Blackwellised particle filters, two-filter smoothers, and particle Gibbs/conditional SMC. Performance is a primary focus of this work, with type stability, GPU-acceleration, and efficient history storage being key design goals.","category":"section"},{"location":"#Interface","page":"Home","title":"Interface","text":"","category":"section"},{"location":"#GeneralisedFilters.AbstractLikelihood","page":"Home","title":"GeneralisedFilters.AbstractLikelihood","text":"AbstractLikelihood\n\nAbstract type for backward likelihood representations used in smoothing and ancestor sampling.\n\nSubtypes represent the predictive likelihood p(y | x) in different forms depending on the state space structure (continuous Gaussian vs discrete).\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.AncestorCallback","page":"Home","title":"GeneralisedFilters.AncestorCallback","text":"AncestorCallback\n\nA callback for sparse ancestry storage, which preallocates and returns a populated  ParticleTree object.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.AuxiliaryResampler","page":"Home","title":"GeneralisedFilters.AuxiliaryResampler","text":"AuxiliaryResampler\n\nA resampling scheme for multistage particle resampling with auxiliary weights\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.BackwardDiscretePredictor","page":"Home","title":"GeneralisedFilters.BackwardDiscretePredictor","text":"BackwardDiscretePredictor <: AbstractBackwardPredictor\n\nAlgorithm to recursively compute the backward likelihood βt(i) = p(y{t:T} | x_t = i) for discrete state space models.\n\nAll computations are performed in log-space using logsumexp for numerical stability. The resulting DiscreteLikelihood stores log β values internally.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.BackwardInformationPredictor","page":"Home","title":"GeneralisedFilters.BackwardInformationPredictor","text":"BackwardInformationPredictor(; jitter=nothing, initial_jitter=nothing)\n\nAn algorithm to recursively compute the predictive likelihood p(y{t:T} | xt) of a linear Gaussian state space model in information form.\n\nFields\n\njitter::Union{Nothing, Real}: Optional value added to the precision matrix Ω after the backward predict step to improve numerical stability. If nothing, no jitter is applied.\ninitial_jitter::Union{Nothing, Real}: Optional value added to the precision matrix Ω at initialization to improve numerical stability.\n\nThis implementation is based on https://arxiv.org/pdf/1505.06357\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.DenseAncestorCallback","page":"Home","title":"GeneralisedFilters.DenseAncestorCallback","text":"DenseAncestorCallback\n\nA callback for dense ancestry storage, which fills a DenseParticleContainer.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.DiscreteFilter","page":"Home","title":"GeneralisedFilters.DiscreteFilter","text":"DiscreteFilter <: AbstractFilter\n\nForward filtering algorithm for discrete (finite) state space models.\n\nComputes the filtered distribution πt(i) = p(xt = i | y_{1:t}) recursively.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.DiscreteLikelihood","page":"Home","title":"GeneralisedFilters.DiscreteLikelihood","text":"DiscreteLikelihood <: AbstractLikelihood\n\nA container representing the backward likelihood β_t(i) = p(y | x = i) for discrete state spaces, stored in log-space for numerical stability.\n\nThis representation is used in backward filtering algorithms for discrete SSMs (HMMs) and Rao-Blackwellised particle filtering with discrete inner states.\n\nFields\n\nlog_β::VT: Vector of log backward likelihoods, where log_β[i] = log p(y | x = i)\n\nSee also\n\nBackwardDiscretePredictor: Algorithm that uses this representation\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.DiscreteSmoother","page":"Home","title":"GeneralisedFilters.DiscreteSmoother","text":"DiscreteSmoother <: AbstractSmoother\n\nA forward-backward smoother for discrete state space models.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.HierarchicalState","page":"Home","title":"GeneralisedFilters.HierarchicalState","text":"A container for a sampled state from a hierarchical SSM, with separation between the outer and inner dimensions. Note this differs from a RBState in the the inner state is a sample rather than a conditional distribution.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.InformationLikelihood","page":"Home","title":"GeneralisedFilters.InformationLikelihood","text":"InformationLikelihood <: AbstractLikelihood\n\nA container representing an unnormalized Gaussian likelihood p(y | x) in information form, parameterized by natural parameters (λ, Ω).\n\nThe unnormalized log-likelihood is given by:     log p(y | x) ∝ λ'x - (1/2)x'Ωx\n\nThis representation is particularly useful in backward filtering algorithms and Rao-Blackwellised particle filtering, where it represents the predictive likelihood p(y{t:T} | xt) conditioned on future observations.\n\nFields\n\nλ::λT: The natural parameter vector (information vector)\nΩ::ΩT: The natural parameter matrix (information/precision matrix)\n\nSee also\n\nnatural_params: Extract the natural parameters (λ, Ω)\nBackwardInformationPredictor: Algorithm that uses this representation\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.KalmanFilter","page":"Home","title":"GeneralisedFilters.KalmanFilter","text":"KalmanFilter(; jitter=nothing)\n\nKalman filter for linear Gaussian state space models.\n\nFields\n\njitter::Union{Nothing, Real}: Optional value added to the covariance matrix after the update step to improve numerical stability. If nothing, no jitter is applied.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.KalmanGradientCache","page":"Home","title":"GeneralisedFilters.KalmanGradientCache","text":"KalmanGradientCache\n\nCache of intermediate values from a Kalman filter update step for gradient computation.\n\nFields\n\nμ_pred: Predicted mean x̂_{n|n-1}\nΣ_pred: Predicted covariance P_{n|n-1}\nμ_filt: Filtered mean x̂_{n|n}\nΣ_filt: Filtered covariance P_{n|n}\nS: Innovation covariance\nK: Kalman gain\nz: Innovation (y - H*μ_pred - c)\nI_KH: I - K*H\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.Particle","page":"Home","title":"GeneralisedFilters.Particle","text":"Particle\n\nA container representing a single particle in a particle filter distribution, composed of a weighted sampled (stored as a log weight) and its ancestor index.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.ParticleDistribution","page":"Home","title":"GeneralisedFilters.ParticleDistribution","text":"ParticleDistribution\n\nA container for particle filters which composes a collection of weighted particles (with their ancestories) into a distibution-like object.\n\nFields\n\nparticles::VT: Vector of weighted particles\nll_baseline::WT: Baseline for computing log-likelihood increment. A scalar that caches the unnormalized logsumexp of weights before update (for standard PF/guided filters) or a modified value that includes APF first-stage correction (for auxiliary PF).\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.ParticleTree","page":"Home","title":"GeneralisedFilters.ParticleTree","text":"ParticleTree\n\nA sparse container for particle ancestry, which tracks the lineage of the filtered draws.\n\nReference\n\nJacob, P., Murray L., & Rubenthaler S. (2015). Path storage in the particle  filter doi:10.1007/s11222-013-9445-x\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.RBState","page":"Home","title":"GeneralisedFilters.RBState","text":"RBState\n\nA container representing a single state with a Rao-Blackwellised component. This differs from a HierarchicalState which contains a sample of the conditionally analytical state rather than the distribution itself.\n\nFields\n\nx::XT: The sampled state component\nz::ZT: The Rao-Blackwellised distribution component\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.ResamplerCallback","page":"Home","title":"GeneralisedFilters.ResamplerCallback","text":"ResamplerCallback\n\nA callback which follows the resampling indices over the filtering algorithm. This is more of a debug tool and visualizer for various resapmling algorithms.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.SRKalmanFilter","page":"Home","title":"GeneralisedFilters.SRKalmanFilter","text":"SRKalmanFilter(; jitter=nothing)\n\nSquare-root Kalman filter for linear Gaussian state space models.\n\nUses QR factorization to propagate the Cholesky factor of the covariance matrix directly, avoiding the numerical instabilities associated with forming and subtracting full covariance matrices.\n\nFields\n\njitter::Union{Nothing, Real}: Optional value added to the covariance matrix after the update step to improve numerical stability. If nothing, no jitter is applied.\n\nAlgorithm\n\nThe SRKF represents the covariance as Σ = U' * U where U is upper triangular.\n\nPredict Step: Given filtered state:\n\nForm matrix A = [[√Q], [A*U']]\nQR factorize to obtain U_new (predicted square-root covariance)\n\nUpdate Step: Given predicted state and observation y:\n\nForm matrix A = [[√R, H*U'], [0, U']]\nQR factorize: Q, B = qr(A) where B is upper triangular\nExtract U_new (posterior square-root covariance) from bottom-right block\nCompute Kalman gain from the factorization components\n\nSee also: KalmanFilter\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.TypelessBaseline","page":"Home","title":"GeneralisedFilters.TypelessBaseline","text":"TypelessBaseline\n\nA lazy promotion for the computation of log-likelihood baslines given a collection of unweighted particles.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters.TypelessZero","page":"Home","title":"GeneralisedFilters.TypelessZero","text":"TypelessZero\n\nA lazy promotion for uninitialized particle weights whos type is not yet known at the first simulation of a particle filter.\n\n\n\n\n\n","category":"type"},{"location":"#GeneralisedFilters._correct_cholesky_sign-Tuple{Any}","page":"Home","title":"GeneralisedFilters._correct_cholesky_sign","text":"_correct_cholesky_sign(R)\n\nEnsure the diagonal of an upper triangular matrix is positive.\n\nQR factorization produces an upper triangular R that may have negative diagonal elements.  For use as a Cholesky factor, the diagonal must be positive. This function multiplies rows by -1 as needed, in an StaticArray-compatible fashion.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.ancestor_weight-Tuple{GeneralisedFilters.Particle, Any, Any, Integer, Any}","page":"Home","title":"GeneralisedFilters.ancestor_weight","text":"ancestor_weight(particle, dyn, algo, iter, ref_state; kwargs...)\n\nCompute the full (unnormalized) log backward sampling weight for a particle.\n\nThis is a convenience function that combines the particle's filtering log-weight with the future conditional density:\n\nlog tildew_tT^(i) = log w_t^(i) + log p(x_t+1T^* y_t+1T mid x_1t^(i) y_1t)\n\nArguments\n\nparticle::Particle: The candidate particle at time t\ndyn: The latent dynamics model\nalgo: The filtering algorithm\niter::Integer: The time step t\nref_state: The reference trajectory state at time t+1\n\nReturns\n\nThe log backward sampling weight (unnormalized).\n\nSee also: future_conditional_density\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_gradient_predict-Tuple{AbstractVector, AbstractMatrix, Any}","page":"Home","title":"GeneralisedFilters.backward_gradient_predict","text":"backward_gradient_predict(∂μ_pred, ∂Σ_pred, A)\n\nPropagate gradients backward through the Kalman predict step (predicted → previous filtered).\n\nThis implements equations 10-11 from Parellier et al.\n\nArguments\n\n∂μ_pred: Gradient of loss w.r.t. predicted mean ∂L/∂x̂_{n|n-1}\n∂Σ_pred: Gradient of loss w.r.t. predicted covariance ∂L/∂P_{n|n-1}\nA: Dynamics matrix at this time step\n\nReturns\n\nA tuple (∂μ_filt_prev, ∂Σ_filt_prev) containing gradients w.r.t. the previous filtered state.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_gradient_update-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache, Any, Any}","page":"Home","title":"GeneralisedFilters.backward_gradient_update","text":"backward_gradient_update(∂μ_filt, ∂Σ_filt, cache, H, R)\n\nPropagate gradients backward through the Kalman update step (filtered → predicted).\n\nThis implements equations 8-9 from Parellier et al., computing the gradients with respect to the predicted state from the gradients with respect to the filtered state.\n\nArguments\n\n∂μ_filt: Gradient of loss w.r.t. filtered mean ∂L/∂x̂_{n|n}\n∂Σ_filt: Gradient of loss w.r.t. filtered covariance ∂L/∂P_{n|n}\ncache: KalmanGradientCache from the forward pass\nH: Observation matrix at this time step\nR: Observation noise covariance at this time step\n\nReturns\n\nA tuple (∂μ_pred, ∂Σ_pred) containing gradients w.r.t. the predicted state.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_initialise","page":"Home","title":"GeneralisedFilters.backward_initialise","text":"backward_initialise(rng, obs, algo, iter, observation; kwargs...)\n\nInitialize the backward likelihood at the final time step T.\n\nThis creates an initial representation of p(yT | xT), the likelihood of the final observation given the state at time T.\n\nArguments\n\nrng: Random number generator\nobs: The observation process model\nalgo: The backward predictor algorithm\niter::Integer: The final time step T\nobservation: The observation y_T at time T\n\nReturns\n\nA representation of the likelihood p(yT | xT).\n\nImplementations\n\nLinear Gaussian (LinearGaussianObservationProcess, BackwardInformationPredictor): Returns InformationLikelihood with λ = H'R⁻¹(y-c) and Ω = H'R⁻¹H\n\nSee also: backward_predict, backward_update\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.backward_initialise-Tuple{Random.AbstractRNG, LinearGaussianObservationProcess, BackwardInformationPredictor, Integer, Any}","page":"Home","title":"GeneralisedFilters.backward_initialise","text":"backward_initialise(rng, obs, algo, iter, y; kwargs...)\n\nInitialise a backward predictor at time T with observation y, forming the likelihood p(yT | xT).\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_initialise-Tuple{Random.AbstractRNG, SSMProblems.ObservationProcess, BackwardDiscretePredictor, Integer, Any}","page":"Home","title":"GeneralisedFilters.backward_initialise","text":"backward_initialise(rng, obs, algo::BackwardDiscretePredictor, iter, y; kwargs...)\n\nInitialize the backward likelihood at time T with observation y_T.\n\nReturns a DiscreteLikelihood where log βT(i) = log p(yT | x_T = i).\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_predict","page":"Home","title":"GeneralisedFilters.backward_predict","text":"backward_predict(rng, dyn, algo, iter, state; kwargs...)\n\nPerform a backward prediction step, propagating the likelihood backward through the dynamics.\n\nGiven a representation of p(y{t+1:T} | x{t+1}), compute p(y{t+1:T} | xt) by marginalizing over the transition:\n\np(y_{t+1:T} | x_t) = ∫ p(x_{t+1} | x_t) p(y_{t+1:T} | x_{t+1}) dx_{t+1}\n\nArguments\n\nrng: Random number generator\ndyn: The latent dynamics model\nalgo: The backward predictor algorithm\niter::Integer: The time step t (predicting from t to t+1)\nstate: The backward likelihood p(y{t+1:T} | x{t+1})\n\nReturns\n\nThe backward likelihood p(y{t+1:T} | xt).\n\nImplementations\n\nLinear Gaussian (LinearGaussianLatentDynamics, BackwardInformationPredictor): Updates InformationLikelihood using the backward information filter equations.\n\nSee also: backward_initialise, backward_update\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.backward_predict-Tuple{Random.AbstractRNG, DiscreteLatentDynamics, BackwardDiscretePredictor, Integer, DiscreteLikelihood}","page":"Home","title":"GeneralisedFilters.backward_predict","text":"backward_predict(rng, dyn, algo::BackwardDiscretePredictor, iter, state; kwargs...)\n\nBackward prediction step: marginalize through dynamics without incorporating observations.\n\nTakes p(y{t+1:T} | x{t+1}) and computes p(y{t+1:T} | xt) by marginalizing over x{t+1}:     p(y{t+1:T} | xt = i) = Σj P{ij} p(y{t+1:T} | x_{t+1} = j)\n\nIn log-space: log p(y{t+1:T} | xt = i) = logsumexpj(log P{ij} + log p(y{t+1:T} | x{t+1} = j))\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_predict-Tuple{Random.AbstractRNG, LinearGaussianLatentDynamics, BackwardInformationPredictor, Integer, InformationLikelihood}","page":"Home","title":"GeneralisedFilters.backward_predict","text":"backward_predict(rng, dyn, algo, iter, state; prev_outer=nothing, next_outer=nothing, kwargs...)\n\nPerform a backward prediction step to compute p(y{t+1:T} | xt) from p(y{t:T} | x{t+1}).\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_smooth","page":"Home","title":"GeneralisedFilters.backward_smooth","text":"backward_smooth(dyn, algo, step, filtered, smoothed_next; predicted=nothing, kwargs...)\n\nPerform a single backward smoothing step, computing the smoothed distribution at time step given the filtered distribution at time step and the smoothed distribution at time step+1.\n\nFor efficiency, the predicted distribution p(x_{t+1} | y_{1:t}) should be provided via the predicted keyword argument if available from the forward pass. If omitted, it will be recomputed internally, which duplicates work.\n\nThis implements the backward recursion of the forward-backward (Rauch-Tung-Striebel) smoother:\n\np(x_t  y_1T) propto p(x_t  y_1t) int fracp(x_t+1  x_t)  p(x_t+1  y_1T)p(x_t+1  y_1t)  dx_t+1\n\nArguments\n\ndyn: The latent dynamics model\nalgo: The filtering algorithm (determines the state representation)\nstep::Integer: The time index t of the filtered state\nfiltered: The filtered distribution p(x_t  y_1t)\nsmoothed_next: The smoothed distribution p(x_t+1  y_1T)\npredicted=nothing: The predicted distribution p(x_t+1  y_1t). Should be provided if available; if nothing, it is recomputed from filtered.\n\nReturns\n\nThe smoothed distribution p(x_t  y_1T).\n\nImplementations\n\nLinear Gaussian (LinearGaussianLatentDynamics, KalmanFilter): Returns MvNormal using the RTS equations with smoothing gain G = Sigma_textfilt A^top Sigma_textpred^-1\n\nSee also: two_filter_smooth, smooth\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.backward_smooth-Tuple{DiscreteLatentDynamics, DiscreteFilter, Integer, AbstractVector, AbstractVector}","page":"Home","title":"GeneralisedFilters.backward_smooth","text":"backward_smooth(dyn, algo::DiscreteFilter, step, filtered, smoothed_next; predicted, kwargs...)\n\nPerform one step of backward smoothing for discrete state spaces.\n\nComputes γt(i) = πt(i) * Σj [P{ij} * γ{t+1}(j) / π̂{t+1}(j)]\n\nwhere:\n\nπ_t(i) is the filtered distribution at time t\nγ_{t+1}(j) is the smoothed distribution at time t+1\nπ̂_{t+1}(j) is the predicted distribution at time t+1\nP_{ij} is the transition probability from state i to state j\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_update","page":"Home","title":"GeneralisedFilters.backward_update","text":"backward_update(obs, algo, iter, state, observation; kwargs...)\n\nIncorporate an observation into the backward likelihood.\n\nGiven p(y{t+1:T} | xt), incorporate observation yt to obtain p(y{t:T} | x_t):\n\np(y_{t:T} | x_t) = p(y_t | x_t) × p(y_{t+1:T} | x_t)\n\nArguments\n\nobs: The observation process model\nalgo: The backward predictor algorithm\niter::Integer: The time step t\nstate: The backward likelihood p(y{t+1:T} | xt)\nobservation: The observation y_t at time t\n\nReturns\n\nThe updated backward likelihood p(y{t:T} | xt).\n\nImplementations\n\nLinear Gaussian (LinearGaussianObservationProcess, BackwardInformationPredictor): Adds observation information: λ += H'R⁻¹(y-c), Ω += H'R⁻¹H\n\nSee also: backward_initialise, backward_predict\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.backward_update-Tuple{LinearGaussianObservationProcess, BackwardInformationPredictor, Integer, InformationLikelihood, Any}","page":"Home","title":"GeneralisedFilters.backward_update","text":"backward_update(obs, algo, iter, state, y; kwargs...)\n\nIncorporate an observation y at time t to compute p(y{t:T} | xt) from p(y{t+1:T} | xt).\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.backward_update-Tuple{SSMProblems.ObservationProcess, BackwardDiscretePredictor, Integer, DiscreteLikelihood, Any}","page":"Home","title":"GeneralisedFilters.backward_update","text":"backward_update(obs, algo::BackwardDiscretePredictor, iter, state, y; kwargs...)\n\nIncorporate observation y_t into the backward likelihood.\n\nUpdates: log β(i) += log p(yt | xt = i)\n\nThis transforms p(y{t+1:T} | xt) into βt = p(y{t:T} | x_t).\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{AbstractVector, DiscreteLikelihood}","page":"Home","title":"GeneralisedFilters.compute_marginal_predictive_likelihood","text":"compute_marginal_predictive_likelihood(forward_dist::AbstractVector, backward_dist::DiscreteLikelihood)\n\nCompute the marginal predictive likelihood p(y{t+1:T} | y{1:t}) for discrete states.\n\nGiven a predicted filtering distribution π{t+1}(i) = p(x{t+1} = i | y{1:t}) and backward likelihood β{t+1}(i) = p(y{t+1:T} | x{t+1} = i), computes:\n\np(y_{t+1:T} | y_{1:t}) = Σ_i π_{t+1}(i) * β_{t+1}(i)\n\nAll computations are performed in log-space for numerical stability.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{Distributions.MvNormal, InformationLikelihood}","page":"Home","title":"GeneralisedFilters.compute_marginal_predictive_likelihood","text":"compute_marginal_predictive_likelihood(forward_dist, backward_dist)\n\nCompute the marginal predictive likelihood p(y{t:T} | y{1:t-1}) given a one-step predicted filtering distribution p(x{t+1} | y{1:t}) and a backward predictive likelihood p(y{t+1:T} | x{t+1}).\n\nThis Gaussian implementation is based on Lemma 1 of https://arxiv.org/pdf/1505.06357\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.filter-Tuple{Random.AbstractRNG, SSMProblems.AbstractStateSpaceModel, GeneralisedFilters.AbstractFilter, AbstractVector}","page":"Home","title":"GeneralisedFilters.filter","text":"filter([rng,] model, algo, observations; callback=nothing, kwargs...)\n\nRun a filtering algorithm on a state-space model.\n\nPerforms sequential Bayesian inference by iterating through observations, calling predict and update at each time step.\n\nArguments\n\nrng::AbstractRNG: Random number generator (optional, defaults to default_rng())\nmodel::AbstractStateSpaceModel: The state-space model to filter\nalgo::AbstractFilter: The filtering algorithm (e.g., KalmanFilter(), BootstrapFilter(N))\nobservations::AbstractVector: Vector of observations y₁:ₜ\n\nKeyword Arguments\n\ncallback: Optional callback for recording intermediate states\nkwargs...: Additional arguments passed to model parameter functions\n\nReturns\n\nA tuple (state, log_likelihood) where:\n\nstate: The final filtered state (algorithm-dependent type)\nlog_likelihood: The total log-marginal likelihood log p(y₁:ₜ)\n\nExample\n\nmodel = create_homogeneous_linear_gaussian_model(μ0, Σ0, A, b, Q, H, c, R)\nstate, ll = filter(model, KalmanFilter(), observations)\n\nSee also: predict, update, step, smooth\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.future_conditional_density-Tuple{SSMProblems.LatentDynamics, GeneralisedFilters.AbstractFilter, Integer, Any, Any}","page":"Home","title":"GeneralisedFilters.future_conditional_density","text":"future_conditional_density(dyn, algo, iter, state, ref_state; kwargs...)\n\nCompute the log conditional density of the future trajectory given the present state:\n\nlog p(x_t+1T y_t+1T mid x_1t y_1t)\n\nup to an additive constant that does not depend on x_t.\n\nThis function is the key computational primitive for backward simulation (BS) and ancestor sampling (AS) algorithms. The full backward sampling weight combines this with the filtering weight:\n\ntildew_tT^(i) propto w_t^(i) cdot p(x_t+1T^* y_t+1T mid x_1t^(i) y_1t)\n\nStandard (Markov) Case\n\nFor Markovian models, the future conditional density factorizes as:\n\np(x_t+1T y_t+1T mid x_1t y_1t) = f(x_t+1 mid x_t) cdot p(y_t+1T mid x_t+1T)\n\nThe second factor is constant across candidate ancestors (since x_t+1T^* is fixed), so this function returns only the transition density:\n\ntextttfuture_conditional_density = log f(x_t+1^* mid x_t)\n\nRao-Blackwellised Case\n\nFor hierarchical models with Rao-Blackwellisation, the marginal outer state process is non-Markov due to the marginalized inner state. The future conditional density becomes:\n\np(u_t+1T y_t+1T mid u_1t y_1t) propto f(u_t+1 mid u_t) cdot p(y_t+1T mid u_1T y_1t)\n\nwhere u denotes the outer (sampled) state. Unlike the Markov case, the second factor depends on the candidate ancestor through u_1t. This is computed via the two-filter formula using the forward filtering distribution and backward predictive likelihood.\n\nArguments\n\ndyn: The latent dynamics model\nalgo: The filtering algorithm\niter::Integer: The time step t\nstate: The candidate state at time t (contains filtering distribution for RB case)\nref_state: The reference trajectory state at time t+1\n\nReturns\n\nThe log future conditional density (up to additive constants independent of x_t).\n\nImplementations\n\nGeneric (LatentDynamics, AbstractFilter): Returns logdensity(dyn, iter, state, ref_state)\nRao-Blackwellised (HierarchicalDynamics, RBPF): Combines outer transition density with marginal predictive likelihood. The ref_state.z must be an AbstractLikelihood (InformationLikelihood for Gaussian inner states, DiscreteLikelihood for discrete inner states).\n\nSee also: compute_marginal_predictive_likelihood, BackwardInformationPredictor, BackwardDiscretePredictor\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_A-Tuple{AbstractVector, AbstractMatrix, AbstractVector, Any, Any}","page":"Home","title":"GeneralisedFilters.gradient_A","text":"gradient_A(∂μ_pred, ∂Σ_pred, μ_prev, Σ_prev, A)\n\nCompute gradient of NLL w.r.t. dynamics matrix A.\n\nDerived via chain rule through μpred = A*μprev + b and Σpred = A*Σprev*A' + Q.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_H-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache, Any}","page":"Home","title":"GeneralisedFilters.gradient_H","text":"gradient_H(∂μ_filt, ∂Σ_filt, cache, Σ_pred)\n\nCompute gradient of NLL w.r.t. observation matrix H.\n\nDerived via chain rule through z = y - Hμ_pred - c, S = HΣ_pred*H' + R, and the update.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_Q-Tuple{AbstractMatrix}","page":"Home","title":"GeneralisedFilters.gradient_Q","text":"gradient_Q(∂Σ_pred)\n\nCompute gradient of NLL w.r.t. process noise covariance Q.\n\nImplements equation 13 from Parellier et al.: ∂L/∂Q = ∂L/∂P_{n|n-1}\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_R-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache}","page":"Home","title":"GeneralisedFilters.gradient_R","text":"gradient_R(∂μ_filt, ∂Σ_filt, cache)\n\nCompute gradient of NLL w.r.t. observation noise covariance R.\n\nImplements equation 14 from Parellier et al.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_b-Tuple{AbstractVector}","page":"Home","title":"GeneralisedFilters.gradient_b","text":"gradient_b(∂μ_pred)\n\nCompute gradient of NLL w.r.t. dynamics offset b.\n\nDerived via chain rule through μpred = A*μprev + b.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_c-Tuple{AbstractVector, KalmanGradientCache}","page":"Home","title":"GeneralisedFilters.gradient_c","text":"gradient_c(∂μ_filt, cache)\n\nCompute gradient of NLL w.r.t. observation offset c.\n\nDerived via chain rule through z = y - H*μ_pred - c.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.gradient_y-Tuple{AbstractVector, KalmanGradientCache}","page":"Home","title":"GeneralisedFilters.gradient_y","text":"gradient_y(∂μ_filt, cache)\n\nCompute gradient of NLL w.r.t. observation y.\n\nImplements equation 12 from Parellier et al.: ∂L/∂y = K'*∂L/∂μ_filt + ∂l/∂y\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.initialise","page":"Home","title":"GeneralisedFilters.initialise","text":"initialise([rng,] model, algo; kwargs...)\n\nPropose an initial state distribution from the prior.\n\nArguments\n\nrng: Random number generator (optional, defaults to default_rng())\nmodel: The state space model (or its prior component)\nalgo: The filtering algorithm\n\nReturns\n\nThe initial state distribution.\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.log_likelihoods-Tuple{DiscreteLikelihood}","page":"Home","title":"GeneralisedFilters.log_likelihoods","text":"log_likelihoods(state::DiscreteLikelihood)\n\nExtract the log backward likelihoods from a DiscreteLikelihood.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.marginalise!-Tuple{GeneralisedFilters.ParticleDistribution, Any}","page":"Home","title":"GeneralisedFilters.marginalise!","text":"marginalise!(state::ParticleDistribution)\n\nCompute the log-likelihood increment and normalize particle weights. This function:\n\nComputes LSE of current (post-observation) log-weights\nCalculates llincrement = LSEafter - ll_baseline\nNormalizes weights by subtracting LSE_after\nResets ll_baseline to 0.0\n\nThe llbaseline field handles both standard particle filter and auxiliary particle filter cases through a single-scalar caching mechanism. For standard PF, llbaseline equals the LSE before adding observation weights. For APF with resampling, it includes first-stage correction terms computed during the APF resampling step.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.natural_params-Tuple{InformationLikelihood}","page":"Home","title":"GeneralisedFilters.natural_params","text":"natural_params(state::InformationLikelihood)\n\nExtract the natural parameters (λ, Ω) from an InformationLikelihood.\n\nReturns a tuple (λ, Ω) where λ is the information vector and Ω is the information/precision matrix.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.predict","page":"Home","title":"GeneralisedFilters.predict","text":"predict([rng,] dyn, algo, iter, state, observation; kwargs...)\n\nPropagate the filtered distribution forward in time using the dynamics model.\n\nArguments\n\nrng: Random number generator (optional)\ndyn: The latent dynamics model\nalgo: The filtering algorithm\niter::Integer: The current time step\nstate: The filtered state distribution at time iter-1\nobservation: The observation (may be used by some proposals)\n\nReturns\n\nThe predicted state distribution at time iter.\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{<:GaussianPrior, <:LinearGaussianLatentDynamics, <:LinearGaussianObservationProcess}, KalmanSmoother, AbstractVector}","page":"Home","title":"GeneralisedFilters.smooth","text":"smooth([rng,] model, algo, observations; t_smooth=1, callback=nothing, kwargs...)\n\nRun a forward-backward smoothing pass to compute the smoothed distribution at time t_smooth.\n\nThis function first runs a forward filtering pass using the Kalman filter, caching all filtered distributions, then performs backward smoothing using the Rauch-Tung-Striebel equations from time T back to t_smooth.\n\nArguments\n\nrng: Random number generator (optional, defaults to default_rng())\nmodel: A linear Gaussian state space model\nalgo: The smoothing algorithm (e.g., KalmanSmoother())\nobservations: Vector of observations y₁, ..., yₜ\n\nKeyword Arguments\n\nt_smooth=1: The time step at which to return the smoothed distribution\ncallback=nothing: Optional callback for the forward filtering pass\n\nReturns\n\nA tuple (smoothed, log_likelihood) where:\n\nsmoothed: The smoothed distribution p(xₜ | y₁:ₜ) at time t_smooth\nlog_likelihood: The total log-likelihood from the forward pass\n\nExample\n\nmodel = create_homogeneous_linear_gaussian_model(μ0, Σ0, A, b, Q, H, c, R)\nsmoothed, ll = smooth(model, KalmanSmoother(), observations)\n\nSee also: backward_smooth, filter\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{<:GeneralisedFilters.DiscretePrior, <:DiscreteLatentDynamics, <:SSMProblems.ObservationProcess}, DiscreteSmoother, AbstractVector}","page":"Home","title":"GeneralisedFilters.smooth","text":"smooth(rng, model::DiscreteStateSpaceModel, algo::DiscreteSmoother, observations; t_smooth=1, kwargs...)\n\nRun forward-backward smoothing for discrete state space models.\n\nReturns the smoothed distribution at time t_smooth and the log-likelihood.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.srkf_predict-Tuple{Distributions.MvNormal, Any}","page":"Home","title":"GeneralisedFilters.srkf_predict","text":"srkf_predict(state, dyn_params)\n\nPerform the square-root Kalman filter predict step.\n\nGiven the filtered state and dynamics parameters (A, b, Q), compute the predicted state using QR factorization.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.srkf_update-Tuple{Distributions.MvNormal, Any, Any, Any}","page":"Home","title":"GeneralisedFilters.srkf_update","text":"srkf_update(state, obs_params, observation, jitter)\n\nPerform the square-root Kalman filter update step.\n\nGiven the predicted state, observation parameters (H, c, R), and observation y, compute the filtered state and log-likelihood using QR factorization.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.step","page":"Home","title":"GeneralisedFilters.step","text":"step([rng,] model, algo, iter, state, observation; kwargs...)\n\nPerform a combined predict and update step of the filtering algorithm.\n\nArguments\n\nrng: Random number generator (optional)\nmodel: The state space model\nalgo: The filtering algorithm\niter::Integer: The current time step\nstate: The current state distribution\nobservation: The observation at time iter\n\nReturns\n\nA tuple (new_state, log_likelihood_increment).\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.two_filter_smooth","page":"Home","title":"GeneralisedFilters.two_filter_smooth","text":"two_filter_smooth(filtered, backward_lik)\n\nCombine a forward filtered distribution with a backward predictive likelihood to obtain the smoothed distribution at a given time step.\n\nThe smoothed distribution is the (normalized) product:\n\np(x_t  y_1T) propto p(x_t  y_1t) times p(y_t+1T  x_t)\n\nwhere:\n\nfiltered represents the forward filtered distribution p(x_t  y_1t)\nbackward_lik represents the backward predictive likelihood p(y_t+1T  x_t)\n\nNote that backward_lik is a likelihood (function of x), not a distribution over x.\n\nArguments\n\nfiltered: The filtered distribution p(x_t  y_1t)\nbackward_lik: The backward predictive likelihood p(y_t+1T  x_t)\n\nReturns\n\nThe smoothed distribution p(x_t  y_1T).\n\nImplementations\n\nLinear Gaussian (MvNormal, InformationLikelihood): Combines using the product of Gaussians formula in information form.\n\nRelation to compute_marginal_predictive_likelihood\n\nBoth functions take the same inputs but compute different quantities:\n\ntwo_filter_smooth returns the distribution p(x_t  y_1T)\ncompute_marginal_predictive_likelihood returns the scalar p(y_t+1T  y_1t)\n\nSee also: backward_smooth, compute_marginal_predictive_likelihood\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.two_filter_smooth-Tuple{AbstractVector, DiscreteLikelihood}","page":"Home","title":"GeneralisedFilters.two_filter_smooth","text":"two_filter_smooth(filtered::AbstractVector, backward_lik::DiscreteLikelihood)\n\nCombine forward filtered distribution with backward likelihood to get smoothed distribution.\n\nReturns the normalized smoothed distribution γt(i) ∝ πt(i) * β_t(i).\n\nAll computations are performed in log-space for numerical stability.\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.update","page":"Home","title":"GeneralisedFilters.update","text":"update(obs, algo, iter, state, observation; kwargs...)\n\nUpdate the predicted distribution given an observation.\n\nArguments\n\nobs: The observation process model\nalgo: The filtering algorithm\niter::Integer: The current time step\nstate: The predicted state distribution\nobservation: The observation at time iter\n\nReturns\n\nA tuple (filtered_state, log_likelihood_increment).\n\n\n\n\n\n","category":"function"},{"location":"#GeneralisedFilters.update_with_cache-Tuple{LinearGaussianObservationProcess, KalmanFilter, Integer, Distributions.MvNormal, AbstractVector}","page":"Home","title":"GeneralisedFilters.update_with_cache","text":"update_with_cache(obs, algo, iter, state, observation; kwargs...)\n\nPerform Kalman update and return cache for gradient computation.\n\nThis extends the standard update step to also return a KalmanGradientCache containing intermediate values needed for efficient backward gradient propagation.\n\nReturns\n\nA tuple (filtered_state, log_likelihood, cache) where:\n\nfiltered_state: The posterior state as MvNormal\nlog_likelihood: The log-likelihood increment\ncache: A KalmanGradientCache for use in backward gradient computation\n\n\n\n\n\n","category":"method"},{"location":"#GeneralisedFilters.will_resample","page":"Home","title":"GeneralisedFilters.will_resample","text":"will_resample(resampler::AbstractResampler, state::ParticleDistribution)\n\nDetermine whether a resampler will trigger resampling given the current particle state. For uncondition resamplers, always returns true. For conditional resamplers (e.g., ESSResampler), checks the resampling condition.\n\n\n\n\n\n","category":"function"}]
}
