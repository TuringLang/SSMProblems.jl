<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · GeneralisedFilters</title><meta name="title" content="Home · GeneralisedFilters"/><meta property="og:title" content="Home · GeneralisedFilters"/><meta property="twitter:title" content="Home · GeneralisedFilters"/><meta name="description" content="Documentation for GeneralisedFilters."/><meta property="og:description" content="Documentation for GeneralisedFilters."/><meta property="twitter:description" content="Documentation for GeneralisedFilters."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body>
<!-- NAVBAR START -->
<style>
    @import url('https://fonts.googleapis.com/css2?family=Source+Sans+Pro&display=swap');

    /* Documenter.jl CSS Overrides */
    html {
        scroll-padding-top: calc(var(--navbar-height) + 1rem);
    }
    .docs-sidebar, #documenter {
        margin-top: var(--navbar-height);
    }
    .docs-version-selector {
        margin-bottom: 60px !important;
    }
    @media screen and (max-width: 1056px) {
        .docs-version-selector {
            margin-bottom: 60px !important;
        }
        .docs-sidebar {
            margin-top: 0 !important;
        }
    }
    /* End of Documenter.jl Tweaks */

    /* Color and Font Variables */
    :root {
        --heading-color: #6c757d;
        --item-color: rgb(165, 165, 165);
        --primary-bg: white;
        --hover-color: #8faad2;
        --deprecated-bg: #ff4d4d;
        --deprecated-text: white;
        --icon-color: #6c757d;
        --shadow-color: rgba(0, 0, 0, 0.1);
        --dropdown-hover-bg: #e9ecef;
        
        /* Typography */
        --font-family: "Source Sans Pro", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
        --nav-link-font-size: 1.0625rem;
        --turing-title-font-size: 21.25px;
        --icon-font-size: 1.25rem;
        --dropdown-arrow-font-size: 0.6875rem;
        --badge-font-size: 0.75rem;

        /* Sizing and Spacing */
        --navbar-height: 3.75rem;
        --logo-height: 31px;
        --logo-width: auto;
        --logo-padding-top: 7px;
        --logo-margin-left: 0.8rem;
        --title-margin-left: 0.4px;
        --title-nav-spacing: 1.1rem;
        --nav-item-margin-left: 1.3rem;
        --icon-margin-left: 1rem;
        --dropdown-padding: 1.875rem;
        --dropdown-item-width: 12.5rem;
        --dropdown-subitem-width: 15.625rem;
        --dropdown-subitem-padding: 0.125rem 0.625rem;
    }

    /* Dark Theme Variable Overrides */
    html.theme--documenter-dark {
        --heading-color: #e0e0e0;
        --item-color: #bdbdbd;
        --primary-bg: #1f2424;
        --hover-color: #ffffff;
        --icon-color: #e0e0e0;
        --shadow-color: rgba(255, 255, 255, 0.1);
        --dropdown-hover-bg: #424242;
    }

    /* Catppuccin Theme Overrides */
    html.theme--catppuccin-latte {
        --heading-color: #4c4f69;
        --primary-bg: #eff1f5;
        --icon-color: #4c4f69;
        --shadow-color: rgba(0, 0, 0, 0.1);
        --dropdown-hover-bg: #e6e9ef;
    }
    html.theme--catppuccin-frappe {
        --heading-color: #c6d0f5;
        --primary-bg: #303446;
        --icon-color: #c6d0f5;
        --shadow-color: rgba(255, 255, 255, 0.1);
        --dropdown-hover-bg: #51576d;
    }
    html.theme--catppuccin-macchiato {
        --heading-color: #cad3f5;
        --primary-bg: #24273a;
        --icon-color: #cad3f5;
        --shadow-color: rgba(255, 255, 255, 0.1);
        --dropdown-hover-bg: #494d64;
    }
    html.theme--catppuccin-mocha {
        --heading-color: #cad3f5;
        --primary-bg: #1e1e2e;
        --icon-color: #cad3f5;
        --shadow-color: rgba(255, 255, 255, 0.1);
        --dropdown-hover-bg: #45475a;
    }


    /* Main Navigation Bar */
    .ext-navigation {
        font-family: var(--font-family);
        position: fixed;
        height: var(--navbar-height);
        top: 0;
        width: 100%;
        background-color: var(--primary-bg);
        z-index: 1000;
        box-shadow: 0 2px 4px var(--shadow-color);
        display: flex;
        align-items: center;
        padding: 0 1.0625rem;
        transition: transform 0.3s, background-color 0.3s;
    }

    nav.ext-navigation .ext-navbar-logo {
        margin-left: var(--logo-margin-left);
        height: auto;
        max-height: var(--logo-height);
        width: auto;
        padding-top: var(--logo-padding-top);
    }
    
    /* Theme-aware logo text color */
    .ext-navbar-logo .logo-text {
        fill: var(--heading-color);
    }
    
    .ext-navbar-title {
        color: var(--heading-color) !important;
        font-size: var(--turing-title-font-size) !important;
        margin-left: var(--title-margin-left);
        text-decoration: none;
        transition: color 0.2s ease;
    }
    
    .ext-navbar-title:hover {
        color: var(--hover-color) !important;
    }

    .ext-nav-links {
        display: flex;
        align-items: center;
        list-style-type: none;
        margin: 0;
        padding: 0;
        flex-grow: 1;
        margin-left: var(--title-nav-spacing);
    }

    .ext-nav-links li:first-child {
        margin-left: 0 !important;
    }

    .ext-nav-links li {
        margin-left: var(--nav-item-margin-left) !important;
    }

    .ext-nav-link {
        color: var(--heading-color) !important;
        text-decoration: none;
        font-size: var(--nav-link-font-size) !important;
        transition: color 0.2s ease;
        cursor: pointer;
    }

    .ext-nav-link:hover,
    .ext-navbar-item-single a:hover,
    .ext-navbar-icons a:hover {
        color: var(--hover-color) !important;
    }

    .ext-navbar-item-single a {
        color: var(--heading-color) !important;
    }
    
    .ext-navbar-icons {
        display: flex;
        align-items: center;
    }

    .ext-navbar-icons a {
        color: var(--icon-color) !important;
        font-size: var(--icon-font-size) !important;
        transition: color 0.2s ease;
        margin-left: var(--icon-margin-left);
    }

    .ext-menu-toggle {
        display: none;
        font-size: 1.5rem;
        color: var(--heading-color);
        cursor: pointer;
    }

    .ext-dropdown {
        display: none;
        grid-template-columns: 1fr 1fr 1fr 1fr;
        padding: var(--dropdown-padding);
        position: absolute;
        top: var(--navbar-height);
        width: 100%;
        left: 0;
        background-color: var(--primary-bg);
        line-height: 1.875rem;
        opacity: 0;
        transition: opacity 0.3s ease-in-out, transform 0.3s ease-in-out, background-color 0.3s;
        transform: translateY(-0.625rem);
        box-shadow: 0 4px 6px var(--shadow-color);
    }

    #library-handler::after {
        content: "▼";
        font-size: var(--dropdown-arrow-font-size);
        margin-left: 0.3125rem;
        transition: transform 0.3s ease-in-out;
    }

    #library-handler.open::after {
        content: "▲";
    }

    .ext-dropdown.show {
        display: grid;
        opacity: 1;
        transform: translateY(0);
    }

    .ext-dropdown ul {
        width: var(--dropdown-item-width);
        margin-bottom: 1.25rem;
    }

    .ext-dropdown ul li {
        text-align: left;
        display: flex;
        align-items: center;
    }

    .ext-dropdown ul a li {
        color: var(--item-color);
        width: var(--dropdown-subitem-width);
        border-radius: 3px;
        padding: var(--dropdown-subitem-padding);
        transition: background-color 0.2s ease;
    }

    .ext-dropdown ul a li:hover {
        background-color: var(--dropdown-hover-bg);
    }

    .ext-dropdown-item-heading {
        color: var(--heading-color);
        text-align: center;
    }

    .deprecated-badge {
        background-color: var(--deprecated-bg);
        color: var(--deprecated-text);
        font-size: var(--badge-font-size);
        padding: .1rem;
        border-radius: 3px;
        margin-left: 0.5rem;
        line-height: 1;
    }

    @media (max-width: 966px) {
        .ext-dropdown {
            grid-template-columns: 1fr 1fr 1fr;
        }
    }

    @media (max-width: 768px) {
        .ext-nav-links {
            display: none;
            flex-direction: column;
            align-items: center;
            width: 100%;
            background-color: var(--primary-bg);
            position: absolute;
            top: var(--navbar-height);
            left: 0;
            padding: 0.625rem 0;
            height: auto;
            overflow-y: auto;
            box-shadow: 0 4px 6px var(--shadow-color);
            margin-left: 0;
        }

        .ext-nav-links.show {
            display: flex;
        }

        .ext-nav-links li {
            margin: 0.625rem 0 !important;
            text-align: center;
        }
        
        .ext-menu-toggle {
            display: block;
            margin-left: auto;
        }

        .ext-navigation.hide {
            transform: translateY(calc(-1 * var(--navbar-height)));
        }

        .ext-dropdown {
            position: static;
            display: block;
            opacity: 1;
            transform: none;
            box-shadow: none;
            grid-template-columns: 1fr;
            padding: 0.625rem;
            text-align: center;
        }

        .ext-dropdown ul {
            width: auto;
            display: inline-block;
            margin: 0 auto 0.3125rem;
            text-align: left;
        }
    }
</style>
<nav class="ext-navigation">
    <a href="https://turinglang.org/">
        <svg width="4333" height="1145" viewBox="0 0 4333 1145" fill="none" xmlns="http://www.w3.org/2000/svg" class="ext-navbar-logo">
            <path class="logo-text" d="M0.44603 193.181V66.9868H663.471V193.181H406.62V898H257.297V193.181H0.44603ZM1097.24 635.874V274.74H1244.13V898H1101.7V787.225H1095.21C1081.14 822.121 1058.01 850.66 1025.82 872.842C993.902 895.024 954.542 906.115 907.744 906.115C866.896 906.115 830.783 897.053 799.403 878.929C768.295 860.534 743.948 833.889 726.365 798.993C708.782 763.826 699.99 721.356 699.99 671.581V274.74H846.878V648.858C846.878 688.353 857.699 719.733 879.34 742.997C900.981 766.261 929.385 777.893 964.551 777.893C986.192 777.893 1007.16 772.618 1027.45 762.068C1047.73 751.518 1064.37 735.828 1077.35 714.999C1090.61 693.899 1097.24 667.524 1097.24 635.874ZM1395.17 898V274.74H1537.6V378.617H1544.09C1555.45 342.639 1574.93 314.911 1602.52 295.434C1630.38 275.687 1662.17 265.813 1697.88 265.813C1705.99 265.813 1715.05 266.219 1725.06 267.031C1735.34 267.572 1743.86 268.518 1750.63 269.871V404.992C1744.4 402.828 1734.53 400.934 1721 399.311C1707.75 397.417 1694.9 396.471 1682.46 396.471C1655.68 396.471 1631.6 402.287 1610.23 413.919C1589.13 425.28 1572.49 441.105 1560.32 461.393C1548.15 481.682 1542.06 505.081 1542.06 531.591V898H1395.17ZM1848.21 898V274.74H1995.1V898H1848.21ZM1922.06 186.283C1898.8 186.283 1878.78 178.573 1862.01 163.154C1845.24 147.464 1836.85 128.664 1836.85 106.752C1836.85 84.5701 1845.24 65.7695 1862.01 50.3503C1878.78 34.6606 1898.8 26.8158 1922.06 26.8158C1945.6 26.8158 1965.61 34.6606 1982.12 50.3503C1998.89 65.7695 2007.27 84.5701 2007.27 106.752C2007.27 128.664 1998.89 147.464 1982.12 163.154C1965.61 178.573 1945.6 186.283 1922.06 186.283ZM2293.04 532.809V898H2146.15V274.74H2286.54V380.646H2293.85C2308.18 345.75 2331.04 318.022 2362.42 297.463C2394.07 276.904 2433.16 266.625 2479.69 266.625C2522.7 266.625 2560.17 275.822 2592.09 294.217C2624.28 312.612 2649.17 339.257 2666.75 374.153C2684.6 409.049 2693.39 451.385 2693.12 501.159V898H2546.24V523.882C2546.24 482.223 2535.41 449.626 2513.77 426.092C2492.4 402.557 2462.78 390.79 2424.91 390.79C2399.21 390.79 2376.35 396.471 2356.34 407.832C2336.59 418.923 2321.03 435.019 2309.67 456.119C2298.58 477.218 2293.04 502.782 2293.04 532.809ZM3113.5 1144.71C3060.75 1144.71 3015.44 1137.54 2977.57 1123.2C2939.7 1109.13 2909.26 1090.2 2886.27 1066.39C2863.28 1042.59 2847.32 1016.21 2838.39 987.269L2970.67 955.213C2976.62 967.386 2985.28 979.424 2996.64 991.327C3008 1003.5 3023.28 1013.51 3042.49 1021.35C3061.97 1029.47 3086.45 1033.53 3115.93 1033.53C3157.59 1033.53 3192.08 1023.38 3219.4 1003.09C3246.73 983.076 3260.39 950.074 3260.39 904.087V786.008H3253.08C3245.51 801.157 3234.42 816.711 3219.81 832.671C3205.47 848.632 3186.4 862.022 3162.6 872.842C3139.06 883.663 3109.44 889.073 3073.73 889.073C3025.85 889.073 2982.44 877.847 2943.48 855.394C2904.8 832.671 2873.96 798.857 2850.97 753.952C2828.24 708.777 2816.88 652.24 2816.88 584.341C2816.88 515.902 2828.24 458.147 2850.97 411.078C2873.96 363.739 2904.93 327.896 2943.89 303.55C2982.84 278.933 3026.26 266.625 3074.14 266.625C3110.66 266.625 3140.69 272.847 3164.22 285.29C3188.03 297.463 3206.96 312.206 3221.03 329.519C3235.09 346.561 3245.78 362.657 3253.08 377.805H3261.2V274.74H3406.06V908.144C3406.06 961.435 3393.34 1005.53 3367.92 1040.42C3342.49 1075.32 3307.73 1101.43 3263.63 1118.74C3219.54 1136.05 3169.5 1144.71 3113.5 1144.71ZM3114.72 773.835C3145.83 773.835 3172.34 766.261 3194.25 751.112C3216.16 735.963 3232.79 714.187 3244.16 685.783C3255.52 657.379 3261.2 623.295 3261.2 583.53C3261.2 544.305 3255.52 509.95 3244.16 480.465C3233.07 450.979 3216.56 428.12 3194.65 411.89C3173.01 395.389 3146.37 387.138 3114.72 387.138C3081.98 387.138 3054.66 395.659 3032.75 412.701C3010.84 429.744 2994.34 453.143 2983.25 482.899C2972.16 512.385 2966.61 545.929 2966.61 583.53C2966.61 621.672 2972.16 655.08 2983.25 683.754C2994.61 712.158 3011.25 734.34 3033.16 750.3C3055.34 765.99 3082.53 773.835 3114.72 773.835ZM3647.08 906.927C3622.47 906.927 3601.37 898.271 3583.78 880.958C3566.2 863.645 3557.54 842.545 3557.82 817.658C3557.54 793.312 3566.2 772.482 3583.78 755.17C3601.37 737.857 3622.47 729.2 3647.08 729.2C3670.89 729.2 3691.58 737.857 3709.17 755.17C3727.02 772.482 3736.08 793.312 3736.35 817.658C3736.08 834.159 3731.75 849.173 3723.37 862.698C3715.25 876.224 3704.43 887.044 3690.91 895.16C3677.65 903.004 3663.04 906.927 3647.08 906.927ZM3888.01 274.74H4034.9V933.708C4034.9 978.613 4026.38 1015.67 4009.33 1044.89C3992.29 1074.1 3967.67 1095.88 3935.48 1110.22C3903.29 1124.55 3864.2 1131.72 3818.22 1131.72C3812.81 1131.72 3807.8 1131.59 3803.2 1131.32C3798.6 1131.32 3793.6 1131.18 3788.19 1130.91V1011.21C3792.25 1011.48 3795.9 1011.62 3799.15 1011.62C3802.39 1011.89 3805.77 1012.02 3809.29 1012.02C3837.42 1012.02 3857.58 1005.12 3869.75 991.327C3881.92 977.801 3888.01 957.918 3888.01 931.679V274.74ZM3961.05 186.283C3937.51 186.283 3917.36 178.573 3900.59 163.154C3884.09 147.464 3875.84 128.664 3875.84 106.752C3875.84 84.5701 3884.09 65.7695 3900.59 50.3503C3917.36 34.6606 3937.51 26.8158 3961.05 26.8158C3984.31 26.8158 4004.19 34.6606 4020.7 50.3503C4037.47 65.7695 4045.85 84.5701 4045.85 106.752C4045.85 128.664 4037.47 147.464 4020.7 163.154C4004.19 178.573 3984.31 186.283 3961.05 186.283ZM4332.83 66.9868V898H4185.94V66.9868H4332.83Z" fill="currentColor"/>
            <path d="M4076 108.5C4076 168.424 4027.42 217 3967.5 217C3907.58 217 3859 168.424 3859 108.5C3859 48.5762 3907.58 0 3967.5 0C4027.42 0 4076 48.5762 4076 108.5Z" fill="#389725"/>
            <path d="M3755 814.5C3755 874.424 3706.42 923 3646.5 923C3586.58 923 3538 874.424 3538 814.5C3538 754.576 3586.58 706 3646.5 706C3706.42 706 3755 754.576 3755 814.5Z" fill="#9457B1"/>
            <path d="M2030 108.5C2030 168.424 1981.42 217 1921.5 217C1861.58 217 1813 168.424 1813 108.5C1813 48.5762 1861.58 0 1921.5 0C1981.42 0 2030 48.5762 2030 108.5Z" fill="#CA3B33"/>
        </svg>
    </a>
    <!-- <a class="ext-navbar-title" href="https://turinglang.org/">Turing.jl</a> -->
    
    <ul class="ext-nav-links">
        <li><a class="ext-nav-link" href="https://turinglang.org/docs/getting-started/">Get Started</a></li>
        <li><a class="ext-nav-link" href="https://turinglang.org/docs/tutorials/">Tutorials</a></li>
        <li><a class="ext-nav-link" href="https://turinglang.org/docs/faq/">FAQ</a></li>
        <li>
            <p class="ext-nav-link" id="library-handler">Libraries</p>
            <div class="ext-dropdown" id="ext-dropdown-items">
                <ul>
                    <li class="ext-dropdown-item-heading">Modelling Languages</li>
                    <a href="https://turinglang.org/DynamicPPL.jl/"><li>DynamicPPL</li></a>
                    <a href="https://turinglang.org/JuliaBUGS.jl/"><li>JuliaBUGS</li></a>
                    <a href="https://turinglang.org/TuringGLM.jl/"><li>TuringGLM</li></a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading">MCMC</li>
                    <a href="https://turinglang.org/AdvancedHMC.jl/"><li>AdvancedHMC</li></a>
                    <a href="https://turinglang.org/AbstractMCMC.jl/"><li>AbstractMCMC</li></a>
                    <a href="https://github.com/theogf/ThermodynamicIntegration.jl"><li>ThermodynamicIntegration</li></a>
                    <a href="https://turinglang.org/AdvancedPS.jl/"><li>AdvancedPS</li></a>
                    <a href="https://turinglang.org/SliceSampling.jl/"><li>SliceSampling</li></a>
                    <a href="https://turinglang.org/EllipticalSliceSampling.jl/"><li>EllipticalSliceSampling</li></a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading">Diagnostics</li>
                    <a href="https://turinglang.org/MCMCChains.jl/"><li>MCMCChains</li></a>
                    <a href="https://turinglang.org/MCMCDiagnosticTools.jl/"><li>MCMCDiagnosticTools</li></a>
                    <a href="https://chalk-lab.github.io/ParetoSmooth.jl/"><li>ParetoSmooth</li></a>
                </ul>
                <ul>
                    <li class="ext-dropdown-item-heading">Gaussian Processes</li>
                    <a href="https://juliagaussianprocesses.github.io/AbstractGPs.jl/"><li>AbstractGPs</li></a>
                    <a href="https://juliagaussianprocesses.github.io/KernelFunctions.jl/"><li>KernelFunctions</li></a>
                    <a href="https://juliagaussianprocesses.github.io/ApproximateGPs.jl/"><li>ApproximateGPs</li></a>
                </ul>
                <ul><li class="ext-dropdown-item-heading ext-navbar-item-single"><a href="https://turinglang.org/Bijectors.jl/">Bijectors</a></li></ul>
                <ul><li class="ext-dropdown-item-heading ext-navbar-item-single"><a href="https://turinglang.org/TuringCallbacks.jl/">TuringCallbacks</a></li></ul>
                <ul><li class="ext-dropdown-item-heading ext-navbar-item-single"><a href="https://turinglang.org/Deprecated/TuringBenchmarking/">TuringBenchmarking</a><span class="deprecated-badge">Deprecated</span></li></ul>
            </div>
        </li>
        <li><a class="ext-nav-link" href="https://turinglang.org/news/">News</a></li>
        <li><a class="ext-nav-link" href="https://turinglang.org/team/">Team</a></li>
    </ul>

    <div class="ext-navbar-icons">
        <a href="https://x.com/TuringLang" aria-label="Turing on X"><i class="fa-brands fa-x-twitter"></i></a>
        <a href="https://discourse.julialang.org/c/domain/probprog/48" aria-label="Turing on Discourse"><i class="fa-brands fa-discourse"></i></a>
        <a href="https://julialang.slack.com/archives/CCYDC34A0" aria-label="Turing on Slack"><i class="fa-brands fa-slack"></i></a>
        <a href="https://github.com/TuringLang/" aria-label="Turing.jl on GitHub"><i class="fa-brands fa-github"></i></a>
    </div>

    <span class="ext-menu-toggle"><i class="fa-solid fa-bars"></i></span>
</nav>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        const menuToggle = document.querySelector(".ext-menu-toggle");
        const navLinks = document.querySelector(".ext-nav-links");
        const nav = document.querySelector(".ext-navigation");
        const libraryHandler = document.getElementById("library-handler");
        const dropdownContainer = document.getElementById("ext-dropdown-items");
        let lastScrollY = window.scrollY;

        function closeDropdown() {
            if (dropdownContainer.classList.contains("show")) {
                libraryHandler.classList.remove("open");
                dropdownContainer.classList.remove("show");
                setTimeout(() => {
                    if (!dropdownContainer.classList.contains("show")) {
                        dropdownContainer.style.display = "none";
                    }
                }, 300);
            }
        }

        function openDropdown() {
            dropdownContainer.style.display = "grid";
            libraryHandler.classList.add("open");
            setTimeout(() => {
                dropdownContainer.classList.add("show");
            }, 10);
        }

        function setAppropriateHeight() {
            if (window.innerWidth <= 768) {
                const viewportHeight = window.innerHeight;
                const navHeight = nav.offsetHeight;
                navLinks.style.maxHeight = `${viewportHeight - navHeight}px`;
                navLinks.style.overflowY = "auto";
            } else {
                navLinks.style.maxHeight = "";
                navLinks.style.overflowY = "";
            }
        }

        menuToggle.addEventListener("click", (event) => {
            event.stopPropagation();
            navLinks.classList.toggle("show");
            if (navLinks.classList.contains("show")) {
                setAppropriateHeight();
                closeDropdown();
                dropdownContainer.style.display = "none";
            }
        });

        libraryHandler.addEventListener("click", (event) => {
            event.stopPropagation();
            event.preventDefault();
            if (dropdownContainer.classList.contains("show")) {
                closeDropdown();
            } else {
                openDropdown();
            }
            setAppropriateHeight();
        });

        // Close all menus if a click is registered outside the navigation bar.
        document.addEventListener("click", (event) => {
            if (!nav.contains(event.target)) {
                navLinks.classList.remove("show");
                closeDropdown();
            }
        });

        // Hide navigation bar on scroll down in mobile view.
        window.addEventListener("scroll", () => {
            if (window.innerWidth <= 768) {
                if (window.scrollY > lastScrollY && window.scrollY > nav.offsetHeight){
                    nav.classList.add("hide");
                } else {
                    nav.classList.remove("hide");
                }
                lastScrollY = window.scrollY;
            }
        });

        window.addEventListener("resize", setAppropriateHeight);
    });
</script>
<!-- NAVBAR END -->


<div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>GeneralisedFilters</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Documentation"><span>Documentation</span></a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="examples/trend-inflation/">Trend Inflation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/TuringLang/SSMProblems.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/TuringLang/SSMProblems.jl/blob/main/GeneralisedFilters/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="GeneralisedFilters"><a class="docs-heading-anchor" href="#GeneralisedFilters">GeneralisedFilters</a><a id="GeneralisedFilters-1"></a><a class="docs-heading-anchor-permalink" href="#GeneralisedFilters" title="Permalink"></a></h1><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>In the <code>julia</code> REPL:</p><pre><code class="language-julia hljs">] add GeneralisedFilters</code></pre><h2 id="Documentation"><a class="docs-heading-anchor" href="#Documentation">Documentation</a><a id="Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#Documentation" title="Permalink"></a></h2><p><code>GeneralisedFilters</code> provides implementations of various filtering and smoothing algorithms for state-space models (SSMs). The goal of the package is to provide a modular and extensible framework for implementing advanced algorithms including Rao-Blackwellised particle filters, two-filter smoothers, and particle Gibbs/conditional SMC. Performance is a primary focus of this work, with type stability, GPU-acceleration, and efficient history storage being key design goals.</p><h3 id="Interface"><a class="docs-heading-anchor" href="#Interface">Interface</a><a id="Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Interface" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="GeneralisedFilters.AbstractLikelihood"><a class="docstring-binding" href="#GeneralisedFilters.AbstractLikelihood"><code>GeneralisedFilters.AbstractLikelihood</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AbstractLikelihood</code></pre><p>Abstract type for backward likelihood representations used in smoothing and ancestor sampling.</p><p>Subtypes represent the predictive likelihood p(y | x) in different forms depending on the state space structure (continuous Gaussian vs discrete).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L173-L180">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.AncestorCallback"><a class="docstring-binding" href="#GeneralisedFilters.AncestorCallback"><code>GeneralisedFilters.AncestorCallback</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AncestorCallback</code></pre><p>A callback for sparse ancestry storage, which preallocates and returns a populated  <code>ParticleTree</code> object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/callbacks.jl#L369-L374">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.AncestorSampling"><a class="docstring-binding" href="#GeneralisedFilters.AncestorSampling"><code>GeneralisedFilters.AncestorSampling</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AncestorSampling &lt;: AbstractTrajectoryRefreshment</code></pre><p>Conditional SMC with ancestor sampling (CSMC-AS / PGAS). At each time step, the reference particle&#39;s ancestor is resampled using backward weights, improving mixing for the full trajectory. For RBPF models, backward predictive likelihoods are computed at the start of each sweep which enable closed form ancestor weights.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L19-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.AuxiliaryResampler"><a class="docstring-binding" href="#GeneralisedFilters.AuxiliaryResampler"><code>GeneralisedFilters.AuxiliaryResampler</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AuxiliaryResampler</code></pre><p>A resampling scheme for multistage particle resampling with auxiliary weights</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/resamplers.jl#L80-L84">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.BackwardDiscretePredictor"><a class="docstring-binding" href="#GeneralisedFilters.BackwardDiscretePredictor"><code>GeneralisedFilters.BackwardDiscretePredictor</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">BackwardDiscretePredictor &lt;: AbstractBackwardPredictor</code></pre><p>Algorithm to recursively compute the backward likelihood β<em>t(i) = p(y</em>{t:T} | x_t = i) for discrete state space models.</p><p>All computations are performed in log-space using logsumexp for numerical stability. The resulting <code>DiscreteLikelihood</code> stores log β values internally.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L54-L62">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.BackwardInformationPredictor"><a class="docstring-binding" href="#GeneralisedFilters.BackwardInformationPredictor"><code>GeneralisedFilters.BackwardInformationPredictor</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">BackwardInformationPredictor(; jitter=nothing, initial_jitter=nothing)</code></pre><p>An algorithm to recursively compute the predictive likelihood p(y<em>{t:T} | x</em>t) of a linear Gaussian state space model in information form.</p><p><strong>Fields</strong></p><ul><li><code>jitter::Union{Nothing, Real}</code>: Optional value added to the precision matrix Ω after the backward predict step to improve numerical stability. If <code>nothing</code>, no jitter is applied.</li><li><code>initial_jitter::Union{Nothing, Real}</code>: Optional value added to the precision matrix Ω at initialization to improve numerical stability.</li></ul><p>This implementation is based on https://arxiv.org/pdf/1505.06357</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman.jl#L256-L269">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.BackwardSimulation"><a class="docstring-binding" href="#GeneralisedFilters.BackwardSimulation"><code>GeneralisedFilters.BackwardSimulation</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">BackwardSimulation &lt;: AbstractTrajectoryRefreshment</code></pre><p>Conditional SMC with backward simulation (CSMC-BS). Runs a full forward filter with particle storage, then samples a trajectory via a backward pass using backward sampling weights. For RBPF models, backward predictive likelihoods are computed on-the-fly during the backward pass.</p><p>Note: requires O(N*T) storage since the full particle history must be retained.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L29-L38">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.CSMCContext"><a class="docstring-binding" href="#GeneralisedFilters.CSMCContext"><code>GeneralisedFilters.CSMCContext</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">CSMCContext &lt;: DynamicPPL.AbstractContext</code></pre><p>A DynamicPPL leaf context that intercepts <code>x ~ SSMTrajectory(...)</code> during model evaluation, capturing the distribution and variable name so the sampler can extract the SSM and run CSMC.</p><p>For non-SSMTrajectory variables, delegates to <code>DefaultContext</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/turing.jl#L3-L10">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.CSMCModel"><a class="docstring-binding" href="#GeneralisedFilters.CSMCModel"><code>GeneralisedFilters.CSMCModel</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">CSMCModel{MT, YT} &lt;: AbstractMCMC.AbstractModel</code></pre><p>Model wrapper for standalone CSMC sampling via the AbstractMCMC interface.</p><p><strong>Fields</strong></p><ul><li><code>ssm::MT</code>: The state-space model</li><li><code>observations::YT</code>: Vector of observations</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L87-L95">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.CSMCState"><a class="docstring-binding" href="#GeneralisedFilters.CSMCState"><code>GeneralisedFilters.CSMCState</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">CSMCState{TT}</code></pre><p>State of a conditional SMC sampler, containing the current reference trajectory.</p><p>The trajectory is an <code>OffsetVector</code> indexed from 0 (matching the prior at time 0). For RBPF, the trajectory contains <code>RBState</code> objects (outer state + inner filtering distribution).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L74-L82">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ConditionalSMC"><a class="docstring-binding" href="#GeneralisedFilters.ConditionalSMC"><code>GeneralisedFilters.ConditionalSMC</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ConditionalSMC{PF, TR} &lt;: AbstractMCMC.AbstractSampler</code></pre><p>Conditional Sequential Monte Carlo sampler with configurable trajectory refreshment.</p><p><strong>Fields</strong></p><ul><li><code>pf::PF</code>: The underlying particle filter (e.g., <code>BF(N)</code>, <code>RBPF(BF(N), KF())</code>)</li><li><code>refreshment::TR</code>: Trajectory refreshment strategy</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">ConditionalSMC(BF(100), AncestorSampling())

# Shorthand constructors
CSMC(BF(100))                           # Vanilla CSMC
CSMCAS(BF(100))                         # CSMC with ancestor sampling
CSMCAS(RBPF(BF(200), KF()))             # Rao-Blackwellised PGAS</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L43-L61">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.DenseAncestorCallback"><a class="docstring-binding" href="#GeneralisedFilters.DenseAncestorCallback"><code>GeneralisedFilters.DenseAncestorCallback</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DenseAncestorCallback</code></pre><p>A callback for dense ancestry storage, which fills a <code>DenseParticleContainer</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/callbacks.jl#L71-L75">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.DiscreteFilter"><a class="docstring-binding" href="#GeneralisedFilters.DiscreteFilter"><code>GeneralisedFilters.DiscreteFilter</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DiscreteFilter &lt;: AbstractFilter</code></pre><p>Forward filtering algorithm for discrete (finite) state space models.</p><p>Computes the filtered distribution π<em>t(i) = p(x</em>t = i | y_{1:t}) recursively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L5-L11">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.DiscreteLikelihood"><a class="docstring-binding" href="#GeneralisedFilters.DiscreteLikelihood"><code>GeneralisedFilters.DiscreteLikelihood</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DiscreteLikelihood &lt;: AbstractLikelihood</code></pre><p>A container representing the backward likelihood β_t(i) = p(y | x = i) for discrete state spaces, stored in log-space for numerical stability.</p><p>This representation is used in backward filtering algorithms for discrete SSMs (HMMs) and Rao-Blackwellised particle filtering with discrete inner states.</p><p><strong>Fields</strong></p><ul><li><code>log_β::VT</code>: Vector of log backward likelihoods, where <code>log_β[i] = log p(y | x = i)</code></li></ul><p><strong>See also</strong></p><ul><li><a href="#GeneralisedFilters.BackwardDiscretePredictor"><code>BackwardDiscretePredictor</code></a>: Algorithm that uses this representation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L221-L235">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.DiscreteSmoother"><a class="docstring-binding" href="#GeneralisedFilters.DiscreteSmoother"><code>GeneralisedFilters.DiscreteSmoother</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DiscreteSmoother &lt;: AbstractSmoother</code></pre><p>A forward-backward smoother for discrete state space models.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L142-L146">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.HierarchicalState"><a class="docstring-binding" href="#GeneralisedFilters.HierarchicalState"><code>GeneralisedFilters.HierarchicalState</code></a> — <span class="docstring-category">Type</span></summary><section><div><p>A container for a sampled state from a hierarchical SSM, with separation between the outer and inner dimensions. Note this differs from a RBState in the the inner state is a sample rather than a conditional distribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/models/hierarchical.jl#L47-L51">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.InformationLikelihood"><a class="docstring-binding" href="#GeneralisedFilters.InformationLikelihood"><code>GeneralisedFilters.InformationLikelihood</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">InformationLikelihood &lt;: AbstractLikelihood</code></pre><p>A container representing an unnormalized Gaussian likelihood p(y | x) in information form, parameterized by natural parameters (λ, Ω).</p><p>The unnormalized log-likelihood is given by:     log p(y | x) ∝ λ&#39;x - (1/2)x&#39;Ωx</p><p>This representation is particularly useful in backward filtering algorithms and Rao-Blackwellised particle filtering, where it represents the predictive likelihood p(y<em>{t:T} | x</em>t) conditioned on future observations.</p><p><strong>Fields</strong></p><ul><li><code>λ::λT</code>: The natural parameter vector (information vector)</li><li><code>Ω::ΩT</code>: The natural parameter matrix (information/precision matrix)</li></ul><p><strong>See also</strong></p><ul><li><a href="#GeneralisedFilters.natural_params-Tuple{InformationLikelihood}"><code>natural_params</code></a>: Extract the natural parameters (λ, Ω)</li><li><a href="#GeneralisedFilters.BackwardInformationPredictor"><code>BackwardInformationPredictor</code></a>: Algorithm that uses this representation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L183-L203">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.KalmanFilter"><a class="docstring-binding" href="#GeneralisedFilters.KalmanFilter"><code>GeneralisedFilters.KalmanFilter</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">KalmanFilter(; jitter=nothing)</code></pre><p>Kalman filter for linear Gaussian state space models.</p><p><strong>Fields</strong></p><ul><li><code>jitter::Union{Nothing, Real}</code>: Optional value added to the covariance matrix after the update step to improve numerical stability. If <code>nothing</code>, no jitter is applied.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman.jl#L8-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.KalmanGradientCache"><a class="docstring-binding" href="#GeneralisedFilters.KalmanGradientCache"><code>GeneralisedFilters.KalmanGradientCache</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">KalmanGradientCache</code></pre><p>Cache of intermediate values from a Kalman filter update step for gradient computation.</p><p><strong>Fields</strong></p><ul><li><code>μ_pred</code>: Predicted mean x̂_{n|n-1}</li><li><code>Σ_pred</code>: Predicted covariance P_{n|n-1}</li><li><code>μ_filt</code>: Filtered mean x̂_{n|n}</li><li><code>Σ_filt</code>: Filtered covariance P_{n|n}</li><li><code>S</code>: Innovation covariance</li><li><code>K</code>: Kalman gain</li><li><code>z</code>: Innovation (y - H*μ_pred - c)</li><li><code>I_KH</code>: I - K*H</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L8-L22">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.NoRefreshment"><a class="docstring-binding" href="#GeneralisedFilters.NoRefreshment"><code>GeneralisedFilters.NoRefreshment</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">NoRefreshment &lt;: AbstractTrajectoryRefreshment</code></pre><p>Vanilla conditional SMC with no trajectory refreshment. Uses <code>filter()</code> directly with a reference trajectory pinned to particle 1.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L11-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParameterisedSSM"><a class="docstring-binding" href="#GeneralisedFilters.ParameterisedSSM"><code>GeneralisedFilters.ParameterisedSSM</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParameterisedSSM(build, observations)</code></pre><p>A parameterised state-space model that maps parameter vectors to concrete SSMs.</p><p><strong>Fields</strong></p><ul><li><code>build</code>: A callable <code>θ -&gt; AbstractStateSpaceModel</code> that constructs an SSM from parameters. Fixed model components should be captured via closure.</li><li><code>observations</code>: The observation sequence y₁:T.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">function build_model(θ, fixed)
    b = θ[1:2]
    dyn = HomogeneousLinearGaussianLatentDynamics(fixed.A, b, fixed.Q)
    return StateSpaceModel(fixed.prior, dyn, fixed.obs)
end

pssm = ParameterisedSSM(θ -&gt; build_model(θ, fixed), observations)
model = pssm.build(θ)  # returns a concrete SSM</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L166-L187">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.Particle"><a class="docstring-binding" href="#GeneralisedFilters.Particle"><code>GeneralisedFilters.Particle</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">Particle</code></pre><p>A container representing a single particle in a particle filter distribution, composed of a weighted sampled (stored as a log weight) and its ancestor index.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L70-L75">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParticleDistribution"><a class="docstring-binding" href="#GeneralisedFilters.ParticleDistribution"><code>GeneralisedFilters.ParticleDistribution</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParticleDistribution</code></pre><p>A container for particle filters which composes a collection of weighted particles (with their ancestories) into a distibution-like object.</p><p><strong>Fields</strong></p><ul><li><code>particles::VT</code>: Vector of weighted particles</li><li><code>ll_baseline::WT</code>: Baseline for computing log-likelihood increment. A scalar that caches the unnormalized logsumexp of weights before update (for standard PF/guided filters) or a modified value that includes APF first-stage correction (for auxiliary PF).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L109-L120">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParticleGibbs"><a class="docstring-binding" href="#GeneralisedFilters.ParticleGibbs"><code>GeneralisedFilters.ParticleGibbs</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParticleGibbs{CS, PS, ADT} &lt;: AbstractMCMC.AbstractSampler</code></pre><p>Particle Gibbs sampler that alternates between a parameter update (e.g., NUTS) and a trajectory update (conditional SMC).</p><p><strong>Fields</strong></p><ul><li><code>csmc::CS</code>: Conditional SMC sampler for trajectory updates (e.g., <code>CSMCAS(RBPF(BF(200), KF()))</code>)</li><li><code>param::PS</code>: Parameter sampler (e.g., <code>AdvancedHMC.NUTS(0.8)</code>)</li><li><code>adtype::ADT</code>: AD backend (<code>ADTypes.AbstractADType</code>). <code>nothing</code> uses AdvancedHMC&#39;s default (ForwardDiff). For HierarchicalSSM models, specify a reverse-mode backend that uses ChainRules (e.g., <code>AutoZygote()</code>). Requires the corresponding package to be loaded.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">
# Regular SSM
ParticleGibbs(CSMC(BF(100)), NUTS(0.8))

# Hierarchical SSM (needs reverse-mode AD for KF rrule)
ParticleGibbs(CSMCAS(RBPF(BF(200), KF())), NUTS(0.8); adtype=AutoZygote())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/particle_gibbs.jl#L8-L30">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParticleGibbsModel"><a class="docstring-binding" href="#GeneralisedFilters.ParticleGibbsModel"><code>GeneralisedFilters.ParticleGibbsModel</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParticleGibbsModel{PT, MT} &lt;: AbstractMCMC.AbstractModel</code></pre><p>Model for particle Gibbs inference, combining a prior on parameters with a parameterised SSM.</p><p><strong>Fields</strong></p><ul><li><code>prior::PT</code>: Prior distribution on θ (any Distributions.jl distribution)</li><li><code>param_model::MT</code>: A <code>ParameterisedSSM</code> mapping θ to a concrete SSM</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">pssm = ParameterisedSSM(θ -&gt; build_model(θ, fixed), observations)
model = ParticleGibbsModel(MvNormal(zeros(d), 4.0*I), pssm)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/particle_gibbs.jl#L42-L56">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParticleGibbsState"><a class="docstring-binding" href="#GeneralisedFilters.ParticleGibbsState"><code>GeneralisedFilters.ParticleGibbsState</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParticleGibbsState{VT, TT, PS, LDT}</code></pre><p>Internal state of the particle Gibbs sampler.</p><p><strong>Fields</strong></p><ul><li><code>θ</code>: Current parameter vector</li><li><code>trajectory</code>: Current reference trajectory (OffsetVector)</li><li><code>param_state</code>: Parameter sampler state (e.g., <code>AdvancedHMC.HMCState</code>)</li><li><code>log_density</code>: <code>AbstractMCMC.LogDensityModel</code> wrapping the SSM log-density (persisted so the trajectory <code>Ref</code> can be updated between steps)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/particle_gibbs.jl#L62-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParticleGibbsTransition"><a class="docstring-binding" href="#GeneralisedFilters.ParticleGibbsTransition"><code>GeneralisedFilters.ParticleGibbsTransition</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParticleGibbsTransition{VT, NT}</code></pre><p>A single transition of the particle Gibbs sampler, containing the parameter values and diagnostics from the parameter sampler.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/particle_gibbs.jl#L81-L86">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ParticleTree"><a class="docstring-binding" href="#GeneralisedFilters.ParticleTree"><code>GeneralisedFilters.ParticleTree</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ParticleTree</code></pre><p>A sparse container for particle ancestry, which tracks the lineage of the filtered draws.</p><p><strong>Reference</strong></p><p>Jacob, P., Murray L., &amp; Rubenthaler S. (2015). Path storage in the particle  filter <a href="https://dx.doi.org/10.1007/s11222-013-9445-x">doi:10.1007/s11222-013-9445-x</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/callbacks.jl#L106-L115">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.RBState"><a class="docstring-binding" href="#GeneralisedFilters.RBState"><code>GeneralisedFilters.RBState</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RBState</code></pre><p>A container representing a single state with a Rao-Blackwellised component. This differs from a <code>HierarchicalState</code> which contains a sample of the conditionally analytical state rather than the distribution itself.</p><p><strong>Fields</strong></p><ul><li><code>x::XT</code>: The sampled state component</li><li><code>z::ZT</code>: The Rao-Blackwellised distribution component</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L93-L103">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ResamplerCallback"><a class="docstring-binding" href="#GeneralisedFilters.ResamplerCallback"><code>GeneralisedFilters.ResamplerCallback</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ResamplerCallback</code></pre><p>A callback which follows the resampling indices over the filtering algorithm. This is more of a debug tool and visualizer for various resapmling algorithms.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/callbacks.jl#L394-L399">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.SRKalmanFilter"><a class="docstring-binding" href="#GeneralisedFilters.SRKalmanFilter"><code>GeneralisedFilters.SRKalmanFilter</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SRKalmanFilter(; jitter=nothing)</code></pre><p>Square-root Kalman filter for linear Gaussian state space models.</p><p>Uses QR factorization to propagate the Cholesky factor of the covariance matrix directly, avoiding the numerical instabilities associated with forming and subtracting full covariance matrices.</p><p><strong>Fields</strong></p><ul><li><code>jitter::Union{Nothing, Real}</code>: Optional value added to the covariance matrix after the update step to improve numerical stability. If <code>nothing</code>, no jitter is applied.</li></ul><p><strong>Algorithm</strong></p><p>The SRKF represents the covariance as <code>Σ = U&#39; * U</code> where <code>U</code> is upper triangular.</p><p><strong>Predict Step</strong>: Given filtered state:</p><ol><li>Form matrix <code>A = [[√Q], [A*U&#39;]]</code></li><li>QR factorize to obtain <code>U_new</code> (predicted square-root covariance)</li></ol><p><strong>Update Step</strong>: Given predicted state and observation <code>y</code>:</p><ol><li>Form matrix <code>A = [[√R, H*U&#39;], [0, U&#39;]]</code></li><li>QR factorize: <code>Q, B = qr(A)</code> where <code>B</code> is upper triangular</li><li>Extract <code>U_new</code> (posterior square-root covariance) from bottom-right block</li><li>Compute Kalman gain from the factorization components</li></ol><p>See also: <a href="#GeneralisedFilters.KalmanFilter"><code>KalmanFilter</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/srkf.jl#L6-L33">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.SSMParameterLogDensity"><a class="docstring-binding" href="#GeneralisedFilters.SSMParameterLogDensity"><code>GeneralisedFilters.SSMParameterLogDensity</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SSMParameterLogDensity(prior, param_model, af, trajectory)
SSMParameterLogDensity(prior, param_model, trajectory)</code></pre><p>Log-density for SSM parameters θ conditioned on a fixed trajectory:</p><pre><code class="language-julia hljs">log p(θ | trajectory, y) ∝ log p(θ) + log p(trajectory, y | θ)</code></pre><p>Implements the <code>LogDensityProblems</code> interface. The <code>trajectory</code> field should be a <code>Ref</code> so it can be mutated between Gibbs iterations.</p><p><strong>Fields</strong></p><ul><li><code>prior</code>: Prior distribution on θ (any Distributions.jl distribution)</li><li><code>param_model</code>: A <code>ParameterisedSSM</code> mapping θ to an SSM</li><li><code>af</code>: Inner analytical filter for HierarchicalSSM (e.g., <code>KalmanFilter()</code>), or <code>nothing</code> for regular SSMs</li><li><code>trajectory</code>: A <code>Ref</code> holding the current reference trajectory (OffsetVector indexed from 0)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L195-L212">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.SSMTrajectory"><a class="docstring-binding" href="#GeneralisedFilters.SSMTrajectory"><code>GeneralisedFilters.SSMTrajectory</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SSMTrajectory{MT, YT} &lt;: ContinuousMultivariateDistribution</code></pre><p>A distribution over state-space model trajectories. Used in Turing <code>@model</code> blocks to mark the trajectory variable for <code>ParticleGibbs</code>:</p><pre><code class="language-julia hljs">@model function my_model(ys, fixed)
    b ~ Normal(0, 1)
    ssm = build_ssm(b, fixed)
    x ~ SSMTrajectory(ssm, ys)
end</code></pre><p>The <code>logpdf</code> computes the joint log-density of the trajectory and observations:</p><ul><li>Regular SSM: <code>log p(x₀) + Σ_t [log p(xₜ|xₜ₋₁) + log p(yₜ|xₜ)]</code></li><li>HierarchicalSSM: outer transitions + inner KF marginal likelihood</li></ul><p>The inner analytical filter (e.g., <code>KF()</code>) is NOT stored here — it&#39;s a property of the sampler (<code>ParticleGibbs</code>), not the trajectory. For <code>logpdf</code> computation during the parameter step, <code>KF()</code> is auto-detected for <code>HierarchicalSSM</code> models.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/ssm_trajectory.jl#L12-L33">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.TypelessBaseline"><a class="docstring-binding" href="#GeneralisedFilters.TypelessBaseline"><code>GeneralisedFilters.TypelessBaseline</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TypelessBaseline</code></pre><p>A lazy promotion for the computation of log-likelihood baslines given a collection of unweighted particles.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L33-L38">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.TypelessZero"><a class="docstring-binding" href="#GeneralisedFilters.TypelessZero"><code>GeneralisedFilters.TypelessZero</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TypelessZero</code></pre><p>A lazy promotion for uninitialized particle weights whos type is not yet known at the first simulation of a particle filter.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L9-L14">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ChainRulesCore.rrule-Tuple{typeof(kf_loglikelihood), Vararg{Any, 9}}"><a class="docstring-binding" href="#ChainRulesCore.rrule-Tuple{typeof(kf_loglikelihood), Vararg{Any, 9}}"><code>ChainRulesCore.rrule</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ChainRulesCore.rrule(::typeof(kf_loglikelihood), μ0, Σ0, As, bs, Qs, Hs, cs, Rs, ys)</code></pre><p>Reverse-mode AD rule for the Kalman filter log-likelihood. The forward pass runs the KF with gradient caching; the pullback computes analytical gradients using the backward recursion from <code>kalman_gradient.jl</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/kalman_rrule.jl#L3-L9">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters._correct_cholesky_sign-Tuple{Any}"><a class="docstring-binding" href="#GeneralisedFilters._correct_cholesky_sign-Tuple{Any}"><code>GeneralisedFilters._correct_cholesky_sign</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">_correct_cholesky_sign(R)</code></pre><p>Ensure the diagonal of an upper triangular matrix is positive.</p><p>QR factorization produces an upper triangular R that may have negative diagonal elements.  For use as a Cholesky factor, the diagonal must be positive. This function multiplies rows by -1 as needed, in an StaticArray-compatible fashion.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/srkf.jl#L74-L82">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters._csmc_sample-Tuple{Random.AbstractRNG, SSMProblems.AbstractStateSpaceModel, ConditionalSMC{&lt;:Any, GeneralisedFilters.NoRefreshment}, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters._csmc_sample-Tuple{Random.AbstractRNG, SSMProblems.AbstractStateSpaceModel, ConditionalSMC{&lt;:Any, GeneralisedFilters.NoRefreshment}, Any, Any}"><code>GeneralisedFilters._csmc_sample</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">_csmc_sample(rng, model, csmc, observations, ref_traj)</code></pre><p>Run one conditional SMC sweep, returning <code>(trajectory, log_likelihood)</code>.</p><p><code>ref_traj</code> is the reference trajectory from the previous iteration (or <code>nothing</code> for the initial unconditional run). For RBPF, this may contain <code>RBState</code> objects; outer states are extracted automatically via <code>_make_ref_state</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/csmc.jl#L204-L212">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters._make_conditioned_ldf-Tuple{DynamicPPL.Model, DynamicPPL.AbstractVarInfo, AbstractPPL.VarName, Any}"><a class="docstring-binding" href="#GeneralisedFilters._make_conditioned_ldf-Tuple{DynamicPPL.Model, DynamicPPL.AbstractVarInfo, AbstractPPL.VarName, Any}"><code>GeneralisedFilters._make_conditioned_ldf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">_make_conditioned_ldf(cond_model, vi, traj_vn, adtype)</code></pre><p>Create a <code>LogDensityFunction</code> for the conditioned model (trajectory fixed) plus a linked VarInfo for parameter recovery after NUTS steps.</p><p>Returns <code>(ldf, cond_vi_linked)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/turing.jl#L74-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters._recover_params-Tuple{Any, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters._recover_params-Tuple{Any, Any, Any}"><code>GeneralisedFilters._recover_params</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">_recover_params(cond_vi_linked, cond_model, θ_new)</code></pre><p>Given a linked VarInfo template and new unconstrained parameters from NUTS, recover the constrained parameter values as a NamedTuple.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/turing.jl#L97-L102">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.ancestor_weight-Tuple{GeneralisedFilters.Particle, Any, Any, Integer, Any}"><a class="docstring-binding" href="#GeneralisedFilters.ancestor_weight-Tuple{GeneralisedFilters.Particle, Any, Any, Integer, Any}"><code>GeneralisedFilters.ancestor_weight</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ancestor_weight(particle, dyn, algo, iter, ref_state; kwargs...)</code></pre><p>Compute the full (unnormalized) log backward sampling weight for a particle.</p><p>This is a convenience function that combines the particle&#39;s filtering log-weight with the future conditional density:</p><p class="math-container">\[\log \tilde{w}_{t|T}^{(i)} = \log w_t^{(i)} + \log p(x_{t+1:T}^*, y_{t+1:T} \mid x_{1:t}^{(i)}, y_{1:t})\]</p><p><strong>Arguments</strong></p><ul><li><code>particle::Particle</code>: The candidate particle at time t</li><li><code>dyn</code>: The latent dynamics model</li><li><code>algo</code>: The filtering algorithm</li><li><code>iter::Integer</code>: The time step t</li><li><code>ref_state</code>: The reference trajectory state at time t+1</li></ul><p><strong>Returns</strong></p><p>The log backward sampling weight (unnormalized).</p><p>See also: <a href="#GeneralisedFilters.future_conditional_density-Tuple{SSMProblems.LatentDynamics, GeneralisedFilters.AbstractFilter, Integer, Any, Any}"><code>future_conditional_density</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/ancestor_sampling.jl#L104-L126">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_gradient_predict-Tuple{AbstractVector, AbstractMatrix, Any}"><a class="docstring-binding" href="#GeneralisedFilters.backward_gradient_predict-Tuple{AbstractVector, AbstractMatrix, Any}"><code>GeneralisedFilters.backward_gradient_predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_gradient_predict(∂μ_pred, ∂Σ_pred, A)</code></pre><p>Propagate gradients backward through the Kalman predict step (predicted → previous filtered).</p><p>This implements equations 10-11 from Parellier et al.</p><p><strong>Arguments</strong></p><ul><li><code>∂μ_pred</code>: Gradient of loss w.r.t. predicted mean ∂L/∂x̂_{n|n-1}</li><li><code>∂Σ_pred</code>: Gradient of loss w.r.t. predicted covariance ∂L/∂P_{n|n-1}</li><li><code>A</code>: Dynamics matrix at this time step</li></ul><p><strong>Returns</strong></p><p>A tuple <code>(∂μ_filt_prev, ∂Σ_filt_prev)</code> containing gradients w.r.t. the previous filtered state.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L117-L131">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_gradient_update-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters.backward_gradient_update-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache, Any, Any}"><code>GeneralisedFilters.backward_gradient_update</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_gradient_update(∂μ_filt, ∂Σ_filt, cache, H, R)</code></pre><p>Propagate gradients backward through the Kalman update step (filtered → predicted).</p><p>This implements equations 8-9 from Parellier et al., computing the gradients with respect to the predicted state from the gradients with respect to the filtered state.</p><p><strong>Arguments</strong></p><ul><li><code>∂μ_filt</code>: Gradient of loss w.r.t. filtered mean ∂L/∂x̂_{n|n}</li><li><code>∂Σ_filt</code>: Gradient of loss w.r.t. filtered covariance ∂L/∂P_{n|n}</li><li><code>cache</code>: <code>KalmanGradientCache</code> from the forward pass</li><li><code>H</code>: Observation matrix at this time step</li><li><code>R</code>: Observation noise covariance at this time step</li></ul><p><strong>Returns</strong></p><p>A tuple <code>(∂μ_pred, ∂Σ_pred)</code> containing gradients w.r.t. the predicted state.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L77-L94">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_initialise"><a class="docstring-binding" href="#GeneralisedFilters.backward_initialise"><code>GeneralisedFilters.backward_initialise</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">backward_initialise(rng, obs, algo, iter, observation; kwargs...)</code></pre><p>Initialize the backward likelihood at the final time step T.</p><p>This creates an initial representation of p(y<em>T | x</em>T), the likelihood of the final observation given the state at time T.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator</li><li><code>obs</code>: The observation process model</li><li><code>algo</code>: The backward predictor algorithm</li><li><code>iter::Integer</code>: The final time step T</li><li><code>observation</code>: The observation y_T at time T</li></ul><p><strong>Returns</strong></p><p>A representation of the likelihood p(y<em>T | x</em>T).</p><p><strong>Implementations</strong></p><ul><li><strong>Linear Gaussian</strong> (<code>LinearGaussianObservationProcess</code>, <code>BackwardInformationPredictor</code>): Returns <code>InformationLikelihood</code> with λ = H&#39;R⁻¹(y-c) and Ω = H&#39;R⁻¹H</li></ul><p>See also: <a href="#GeneralisedFilters.backward_predict"><code>backward_predict</code></a>, <a href="#GeneralisedFilters.backward_update"><code>backward_update</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L163-L186">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_initialise-Tuple{Random.AbstractRNG, LinearGaussianObservationProcess, BackwardInformationPredictor, Integer, Any}"><a class="docstring-binding" href="#GeneralisedFilters.backward_initialise-Tuple{Random.AbstractRNG, LinearGaussianObservationProcess, BackwardInformationPredictor, Integer, Any}"><code>GeneralisedFilters.backward_initialise</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_initialise(rng, obs, algo, iter, y; kwargs...)</code></pre><p>Initialise a backward predictor at time <code>T</code> with observation <code>y</code>, forming the likelihood p(y<em>T | x</em>T).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman.jl#L278-L283">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_initialise-Tuple{Random.AbstractRNG, SSMProblems.ObservationProcess, BackwardDiscretePredictor, Integer, Any}"><a class="docstring-binding" href="#GeneralisedFilters.backward_initialise-Tuple{Random.AbstractRNG, SSMProblems.ObservationProcess, BackwardDiscretePredictor, Integer, Any}"><code>GeneralisedFilters.backward_initialise</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_initialise(rng, obs, algo::BackwardDiscretePredictor, iter, y; kwargs...)</code></pre><p>Initialize the backward likelihood at time T with observation y_T.</p><p>Returns a <code>DiscreteLikelihood</code> where log β<em>T(i) = log p(y</em>T | x_T = i).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L65-L71">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_predict"><a class="docstring-binding" href="#GeneralisedFilters.backward_predict"><code>GeneralisedFilters.backward_predict</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">backward_predict(rng, dyn, algo, iter, state; kwargs...)</code></pre><p>Perform a backward prediction step, propagating the likelihood backward through the dynamics.</p><p>Given a representation of p(y<em>{t+1:T} | x</em>{t+1}), compute p(y<em>{t+1:T} | x</em>t) by marginalizing over the transition:</p><pre><code class="language-julia hljs">p(y_{t+1:T} | x_t) = ∫ p(x_{t+1} | x_t) p(y_{t+1:T} | x_{t+1}) dx_{t+1}</code></pre><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator</li><li><code>dyn</code>: The latent dynamics model</li><li><code>algo</code>: The backward predictor algorithm</li><li><code>iter::Integer</code>: The time step t (predicting from t to t+1)</li><li><code>state</code>: The backward likelihood p(y<em>{t+1:T} | x</em>{t+1})</li></ul><p><strong>Returns</strong></p><p>The backward likelihood p(y<em>{t+1:T} | x</em>t).</p><p><strong>Implementations</strong></p><ul><li><strong>Linear Gaussian</strong> (<code>LinearGaussianLatentDynamics</code>, <code>BackwardInformationPredictor</code>): Updates <code>InformationLikelihood</code> using the backward information filter equations.</li></ul><p>See also: <a href="#GeneralisedFilters.backward_initialise"><code>backward_initialise</code></a>, <a href="#GeneralisedFilters.backward_update"><code>backward_update</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L189-L214">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_predict-Tuple{Random.AbstractRNG, DiscreteLatentDynamics, BackwardDiscretePredictor, Integer, DiscreteLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.backward_predict-Tuple{Random.AbstractRNG, DiscreteLatentDynamics, BackwardDiscretePredictor, Integer, DiscreteLikelihood}"><code>GeneralisedFilters.backward_predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_predict(rng, dyn, algo::BackwardDiscretePredictor, iter, state; kwargs...)</code></pre><p>Backward prediction step: marginalize through dynamics without incorporating observations.</p><p>Takes p(y<em>{t+1:T} | x</em>{t+1}) and computes p(y<em>{t+1:T} | x</em>t) by marginalizing over x<em>{t+1}:     p(y</em>{t+1:T} | x<em>t = i) = Σ</em>j P<em>{ij} p(y</em>{t+1:T} | x_{t+1} = j)</p><p>In log-space: log p(y<em>{t+1:T} | x</em>t = i) = logsumexp<em>j(log P</em>{ij} + log p(y<em>{t+1:T} | x</em>{t+1} = j))</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L85-L94">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_predict-Tuple{Random.AbstractRNG, LinearGaussianLatentDynamics, BackwardInformationPredictor, Integer, InformationLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.backward_predict-Tuple{Random.AbstractRNG, LinearGaussianLatentDynamics, BackwardInformationPredictor, Integer, InformationLikelihood}"><code>GeneralisedFilters.backward_predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_predict(rng, dyn, algo, iter, state; prev_outer=nothing, next_outer=nothing, kwargs...)</code></pre><p>Perform a backward prediction step to compute p(y<em>{t+1:T} | x</em>t) from p(y<em>{t:T} | x</em>{t+1}).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman.jl#L306-L310">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_smooth"><a class="docstring-binding" href="#GeneralisedFilters.backward_smooth"><code>GeneralisedFilters.backward_smooth</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">backward_smooth(dyn, algo, step, filtered, smoothed_next; predicted=nothing, kwargs...)</code></pre><p>Perform a single backward smoothing step, computing the smoothed distribution at time <code>step</code> given the filtered distribution at time <code>step</code> and the smoothed distribution at time <code>step+1</code>.</p><p>For efficiency, the predicted distribution <code>p(x_{t+1} | y_{1:t})</code> should be provided via the <code>predicted</code> keyword argument if available from the forward pass. If omitted, it will be recomputed internally, which duplicates work.</p><p>This implements the backward recursion of the forward-backward (Rauch-Tung-Striebel) smoother:</p><p class="math-container">\[p(x_t | y_{1:T}) \propto p(x_t | y_{1:t}) \int \frac{p(x_{t+1} | x_t) \, p(x_{t+1} | y_{1:T})}{p(x_{t+1} | y_{1:t})} \, dx_{t+1}\]</p><p><strong>Arguments</strong></p><ul><li><code>dyn</code>: The latent dynamics model</li><li><code>algo</code>: The filtering algorithm (determines the state representation)</li><li><code>step::Integer</code>: The time index t of the filtered state</li><li><code>filtered</code>: The filtered distribution <span>$p(x_t | y_{1:t})$</span></li><li><code>smoothed_next</code>: The smoothed distribution <span>$p(x_{t+1} | y_{1:T})$</span></li><li><code>predicted=nothing</code>: The predicted distribution <span>$p(x_{t+1} | y_{1:t})$</span>. Should be provided if available; if <code>nothing</code>, it is recomputed from <code>filtered</code>.</li></ul><p><strong>Returns</strong></p><p>The smoothed distribution <span>$p(x_t | y_{1:T})$</span>.</p><p><strong>Implementations</strong></p><ul><li><strong>Linear Gaussian</strong> (<code>LinearGaussianLatentDynamics</code>, <code>KalmanFilter</code>): Returns <code>MvNormal</code> using the RTS equations with smoothing gain <span>$G = \Sigma_{\text{filt}} A^\top \Sigma_{\text{pred}}^{-1}$</span></li></ul><p>See also: <a href="#GeneralisedFilters.two_filter_smooth"><code>two_filter_smooth</code></a>, <a href="#GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{&lt;:GaussianPrior, &lt;:LinearGaussianLatentDynamics, &lt;:LinearGaussianObservationProcess}, KalmanSmoother, AbstractVector}"><code>smooth</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L85-L118">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_smooth-Tuple{DiscreteLatentDynamics, DiscreteFilter, Integer, AbstractVector, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.backward_smooth-Tuple{DiscreteLatentDynamics, DiscreteFilter, Integer, AbstractVector, AbstractVector}"><code>GeneralisedFilters.backward_smooth</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_smooth(dyn, algo::DiscreteFilter, step, filtered, smoothed_next; predicted, kwargs...)</code></pre><p>Perform one step of backward smoothing for discrete state spaces.</p><p>Computes γ<em>t(i) = π</em>t(i) * Σ<em>j [P</em>{ij} * γ<em>{t+1}(j) / π̂</em>{t+1}(j)]</p><p>where:</p><ul><li>π_t(i) is the filtered distribution at time t</li><li>γ_{t+1}(j) is the smoothed distribution at time t+1</li><li>π̂_{t+1}(j) is the predicted distribution at time t+1</li><li>P_{ij} is the transition probability from state i to state j</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L149-L161">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_update"><a class="docstring-binding" href="#GeneralisedFilters.backward_update"><code>GeneralisedFilters.backward_update</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">backward_update(obs, algo, iter, state, observation; kwargs...)</code></pre><p>Incorporate an observation into the backward likelihood.</p><p>Given p(y<em>{t+1:T} | x</em>t), incorporate observation y<em>t to obtain p(y</em>{t:T} | x_t):</p><pre><code class="language-julia hljs">p(y_{t:T} | x_t) = p(y_t | x_t) × p(y_{t+1:T} | x_t)</code></pre><p><strong>Arguments</strong></p><ul><li><code>obs</code>: The observation process model</li><li><code>algo</code>: The backward predictor algorithm</li><li><code>iter::Integer</code>: The time step t</li><li><code>state</code>: The backward likelihood p(y<em>{t+1:T} | x</em>t)</li><li><code>observation</code>: The observation y_t at time t</li></ul><p><strong>Returns</strong></p><p>The updated backward likelihood p(y<em>{t:T} | x</em>t).</p><p><strong>Implementations</strong></p><ul><li><strong>Linear Gaussian</strong> (<code>LinearGaussianObservationProcess</code>, <code>BackwardInformationPredictor</code>): Adds observation information: λ += H&#39;R⁻¹(y-c), Ω += H&#39;R⁻¹H</li></ul><p>See also: <a href="#GeneralisedFilters.backward_initialise"><code>backward_initialise</code></a>, <a href="#GeneralisedFilters.backward_predict"><code>backward_predict</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L217-L241">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_update-Tuple{LinearGaussianObservationProcess, BackwardInformationPredictor, Integer, InformationLikelihood, Any}"><a class="docstring-binding" href="#GeneralisedFilters.backward_update-Tuple{LinearGaussianObservationProcess, BackwardInformationPredictor, Integer, InformationLikelihood, Any}"><code>GeneralisedFilters.backward_update</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_update(obs, algo, iter, state, y; kwargs...)</code></pre><p>Incorporate an observation <code>y</code> at time <code>t</code> to compute p(y<em>{t:T} | x</em>t) from p(y<em>{t+1:T} | x</em>t).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman.jl#L344-L348">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.backward_update-Tuple{SSMProblems.ObservationProcess, BackwardDiscretePredictor, Integer, DiscreteLikelihood, Any}"><a class="docstring-binding" href="#GeneralisedFilters.backward_update-Tuple{SSMProblems.ObservationProcess, BackwardDiscretePredictor, Integer, DiscreteLikelihood, Any}"><code>GeneralisedFilters.backward_update</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">backward_update(obs, algo::BackwardDiscretePredictor, iter, state, y; kwargs...)</code></pre><p>Incorporate observation y_t into the backward likelihood.</p><p>Updates: log β(i) += log p(y<em>t | x</em>t = i)</p><p>This transforms p(y<em>{t+1:T} | x</em>t) into β<em>t = p(y</em>{t:T} | x_t).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L114-L122">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{AbstractVector, DiscreteLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{AbstractVector, DiscreteLikelihood}"><code>GeneralisedFilters.compute_marginal_predictive_likelihood</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">compute_marginal_predictive_likelihood(forward_dist::AbstractVector, backward_dist::DiscreteLikelihood)</code></pre><p>Compute the marginal predictive likelihood p(y<em>{t+1:T} | y</em>{1:t}) for discrete states.</p><p>Given a predicted filtering distribution π<em>{t+1}(i) = p(x</em>{t+1} = i | y<em>{1:t}) and backward likelihood β</em>{t+1}(i) = p(y<em>{t+1:T} | x</em>{t+1} = i), computes:</p><pre><code class="language-julia hljs">p(y_{t+1:T} | y_{1:t}) = Σ_i π_{t+1}(i) * β_{t+1}(i)</code></pre><p>All computations are performed in log-space for numerical stability.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/ancestor_sampling.jl#L156-L167">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{Distributions.MvNormal, InformationLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{Distributions.MvNormal, InformationLikelihood}"><code>GeneralisedFilters.compute_marginal_predictive_likelihood</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">compute_marginal_predictive_likelihood(forward_dist, backward_dist)</code></pre><p>Compute the marginal predictive likelihood p(y<em>{t:T} | y</em>{1:t-1}) given a one-step predicted filtering distribution p(x<em>{t+1} | y</em>{1:t}) and a backward predictive likelihood p(y<em>{t+1:T} | x</em>{t+1}).</p><p>This Gaussian implementation is based on Lemma 1 of https://arxiv.org/pdf/1505.06357</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/ancestor_sampling.jl#L132-L140">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.filter-Tuple{Random.AbstractRNG, SSMProblems.AbstractStateSpaceModel, GeneralisedFilters.AbstractFilter, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.filter-Tuple{Random.AbstractRNG, SSMProblems.AbstractStateSpaceModel, GeneralisedFilters.AbstractFilter, AbstractVector}"><code>GeneralisedFilters.filter</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">filter([rng,] model, algo, observations; callback=nothing, kwargs...)</code></pre><p>Run a filtering algorithm on a state-space model.</p><p>Performs sequential Bayesian inference by iterating through observations, calling <a href="#GeneralisedFilters.predict"><code>predict</code></a> and <a href="#GeneralisedFilters.update"><code>update</code></a> at each time step.</p><p><strong>Arguments</strong></p><ul><li><code>rng::AbstractRNG</code>: Random number generator (optional, defaults to <code>default_rng()</code>)</li><li><code>model::AbstractStateSpaceModel</code>: The state-space model to filter</li><li><code>algo::AbstractFilter</code>: The filtering algorithm (e.g., <code>KalmanFilter()</code>, <code>BootstrapFilter(N)</code>)</li><li><code>observations::AbstractVector</code>: Vector of observations y₁:ₜ</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>callback</code>: Optional callback for recording intermediate states</li><li><code>kwargs...</code>: Additional arguments passed to model parameter functions</li></ul><p><strong>Returns</strong></p><p>A tuple <code>(state, log_likelihood)</code> where:</p><ul><li><code>state</code>: The final filtered state (algorithm-dependent type)</li><li><code>log_likelihood</code>: The total log-marginal likelihood log p(y₁:ₜ)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">model = create_homogeneous_linear_gaussian_model(μ0, Σ0, A, b, Q, H, c, R)
state, ll = filter(model, KalmanFilter(), observations)</code></pre><p>See also: <a href="#GeneralisedFilters.predict"><code>predict</code></a>, <a href="#GeneralisedFilters.update"><code>update</code></a>, <a href="#GeneralisedFilters.step"><code>step</code></a>, <a href="#GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{&lt;:GaussianPrior, &lt;:LinearGaussianLatentDynamics, &lt;:LinearGaussianObservationProcess}, KalmanSmoother, AbstractVector}"><code>smooth</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/GeneralisedFilters.jl#L29-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.future_conditional_density-Tuple{SSMProblems.LatentDynamics, GeneralisedFilters.AbstractFilter, Integer, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters.future_conditional_density-Tuple{SSMProblems.LatentDynamics, GeneralisedFilters.AbstractFilter, Integer, Any, Any}"><code>GeneralisedFilters.future_conditional_density</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">future_conditional_density(dyn, algo, iter, state, ref_state; kwargs...)</code></pre><p>Compute the log conditional density of the future trajectory given the present state:</p><p class="math-container">\[\log p(x_{t+1:T}, y_{t+1:T} \mid x_{1:t}, y_{1:t})\]</p><p>up to an additive constant that does not depend on <span>$x_t$</span>.</p><p>This function is the key computational primitive for backward simulation (BS) and ancestor sampling (AS) algorithms. The full backward sampling weight combines this with the filtering weight:</p><p class="math-container">\[\tilde{w}_{t|T}^{(i)} \propto w_t^{(i)} \cdot p(x_{t+1:T}^*, y_{t+1:T} \mid x_{1:t}^{(i)}, y_{1:t})\]</p><p><strong>Standard (Markov) Case</strong></p><p>For Markovian models, the future conditional density factorizes as:</p><p class="math-container">\[p(x_{t+1:T}, y_{t+1:T} \mid x_{1:t}, y_{1:t}) = f(x_{t+1} \mid x_t) \cdot p(y_{t+1:T} \mid x_{t+1:T})\]</p><p>The second factor is constant across candidate ancestors (since <span>$x_{t+1:T}^*$</span> is fixed), so this function returns only the transition density:</p><p class="math-container">\[\texttt{future\_conditional\_density} = \log f(x_{t+1}^* \mid x_t)\]</p><p><strong>Rao-Blackwellised Case</strong></p><p>For hierarchical models with Rao-Blackwellisation, the marginal outer state process is non-Markov due to the marginalized inner state. The future conditional density becomes:</p><p class="math-container">\[p(u_{t+1:T}, y_{t+1:T} \mid u_{1:t}, y_{1:t}) \propto f(u_{t+1} \mid u_t) \cdot p(y_{t+1:T} \mid u_{1:T}, y_{1:t})\]</p><p>where <span>$u$</span> denotes the outer (sampled) state. Unlike the Markov case, the second factor depends on the candidate ancestor through <span>$u_{1:t}$</span>. This is computed via the two-filter formula using the forward filtering distribution and backward predictive likelihood.</p><p><strong>Arguments</strong></p><ul><li><code>dyn</code>: The latent dynamics model</li><li><code>algo</code>: The filtering algorithm</li><li><code>iter::Integer</code>: The time step t</li><li><code>state</code>: The candidate state at time t (contains filtering distribution for RB case)</li><li><code>ref_state</code>: The reference trajectory state at time t+1</li></ul><p><strong>Returns</strong></p><p>The log future conditional density (up to additive constants independent of <span>$x_t$</span>).</p><p><strong>Implementations</strong></p><ul><li><strong>Generic</strong> (<code>LatentDynamics</code>, <code>AbstractFilter</code>): Returns <code>logdensity(dyn, iter, state, ref_state)</code></li><li><strong>Rao-Blackwellised</strong> (<code>HierarchicalDynamics</code>, <code>RBPF</code>): Combines outer transition density with marginal predictive likelihood. The <code>ref_state.z</code> must be an <code>AbstractLikelihood</code> (<code>InformationLikelihood</code> for Gaussian inner states, <code>DiscreteLikelihood</code> for discrete inner states).</li></ul><p>See also: <a href="#GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{AbstractVector, DiscreteLikelihood}"><code>compute_marginal_predictive_likelihood</code></a>, <a href="#GeneralisedFilters.BackwardInformationPredictor"><code>BackwardInformationPredictor</code></a>, <a href="#GeneralisedFilters.BackwardDiscretePredictor"><code>BackwardDiscretePredictor</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/ancestor_sampling.jl#L6-L63">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_A-Tuple{AbstractVector, AbstractMatrix, AbstractVector, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_A-Tuple{AbstractVector, AbstractMatrix, AbstractVector, Any, Any}"><code>GeneralisedFilters.gradient_A</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_A(∂μ_pred, ∂Σ_pred, μ_prev, Σ_prev, A)</code></pre><p>Compute gradient of NLL w.r.t. dynamics matrix A.</p><p>Derived via chain rule through μ<em>pred = A*μ</em>prev + b and Σ<em>pred = A*Σ</em>prev*A&#39; + Q.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L185-L191">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_H-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_H-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache, Any, Any}"><code>GeneralisedFilters.gradient_H</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_H(∂μ_filt, ∂Σ_filt, cache, Σ_pred, H)</code></pre><p>Compute gradient of NLL w.r.t. observation matrix H.</p><p>Derived via chain rule using the information form P<em>filt⁻¹ = P</em>pred⁻¹ + H&#39;R⁻¹H to decouple P_filt from K, then tracing H&#39;s effect through:</p><ul><li>NLL local term (via z and S)</li><li>Filtered mean (via z = y - Hμ<em>pred - c, and K = P</em>filt H&#39; R⁻¹)</li><li>Filtered covariance (via the information form)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L210-L220">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_Q-Tuple{AbstractMatrix}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_Q-Tuple{AbstractMatrix}"><code>GeneralisedFilters.gradient_Q</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_Q(∂Σ_pred)</code></pre><p>Compute gradient of NLL w.r.t. process noise covariance Q.</p><p>Implements equation 13 from Parellier et al.: ∂L/∂Q = ∂L/∂P_{n|n-1}</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L140-L146">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_R-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_R-Tuple{AbstractVector, AbstractMatrix, KalmanGradientCache}"><code>GeneralisedFilters.gradient_R</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_R(∂μ_filt, ∂Σ_filt, cache)</code></pre><p>Compute gradient of NLL w.r.t. observation noise covariance R.</p><p>Implements equation 14 from Parellier et al.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L151-L157">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_b-Tuple{AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_b-Tuple{AbstractVector}"><code>GeneralisedFilters.gradient_b</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_b(∂μ_pred)</code></pre><p>Compute gradient of NLL w.r.t. dynamics offset b.</p><p>Derived via chain rule through μ<em>pred = A*μ</em>prev + b.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L199-L205">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_c-Tuple{AbstractVector, KalmanGradientCache}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_c-Tuple{AbstractVector, KalmanGradientCache}"><code>GeneralisedFilters.gradient_c</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_c(∂μ_filt, cache)</code></pre><p>Compute gradient of NLL w.r.t. observation offset c.</p><p>Derived via chain rule through z = y - H*μ_pred - c.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L244-L250">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.gradient_y-Tuple{AbstractVector, KalmanGradientCache}"><a class="docstring-binding" href="#GeneralisedFilters.gradient_y-Tuple{AbstractVector, KalmanGradientCache}"><code>GeneralisedFilters.gradient_y</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gradient_y(∂μ_filt, cache)</code></pre><p>Compute gradient of NLL w.r.t. observation y.</p><p>Implements equation 12 from Parellier et al.: ∂L/∂y = K&#39;*∂L/∂μ_filt + ∂l/∂y</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L172-L178">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.initialise"><a class="docstring-binding" href="#GeneralisedFilters.initialise"><code>GeneralisedFilters.initialise</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">initialise([rng,] model, algo; kwargs...)</code></pre><p>Propose an initial state distribution from the prior.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator (optional, defaults to <code>default_rng()</code>)</li><li><code>model</code>: The state space model (or its prior component)</li><li><code>algo</code>: The filtering algorithm</li></ul><p><strong>Returns</strong></p><p>The initial state distribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L5-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.inner_loglikelihood"><a class="docstring-binding" href="#GeneralisedFilters.inner_loglikelihood"><code>GeneralisedFilters.inner_loglikelihood</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">inner_loglikelihood(af::AbstractFilter, inner_model, outer_trajectory, observations)</code></pre><p>Compute the marginal log-likelihood log p(y₁:T | u₀:T) of the inner model conditioned on the outer trajectory, using the analytical filter <code>af</code>.</p><p>Dispatches on the filter type to select the appropriate algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L66-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.inner_loglikelihood-Tuple{KalmanFilter, SSMProblems.StateSpaceModel, Any, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.inner_loglikelihood-Tuple{KalmanFilter, SSMProblems.StateSpaceModel, Any, AbstractVector}"><code>GeneralisedFilters.inner_loglikelihood</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">inner_loglikelihood(af::KalmanFilter, inner_model, outer_trajectory, observations)</code></pre><p>KalmanFilter specialization: extracts linear-Gaussian parameters at each timestep and delegates to <code>kf_loglikelihood</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L76-L81">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.kf_loglikelihood-NTuple{9, Any}"><a class="docstring-binding" href="#GeneralisedFilters.kf_loglikelihood-NTuple{9, Any}"><code>GeneralisedFilters.kf_loglikelihood</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">kf_loglikelihood(μ0, Σ0, As, bs, Qs, Hs, cs, Rs, ys)</code></pre><p>Compute the marginal log-likelihood of observations under a linear-Gaussian model via the Kalman filter forward pass.</p><p>Accepts PDMat natively for <code>Σ0</code>, <code>Qs</code>, <code>Rs</code>. A <code>ChainRulesCore.rrule</code> is registered for this function to enable efficient reverse-mode AD gradients using the analytical backward recursion from <code>kalman_gradient.jl</code>.</p><p><strong>Arguments</strong></p><ul><li><code>μ0</code>: Initial mean vector</li><li><code>Σ0</code>: Initial covariance (AbstractPDMat or AbstractMatrix)</li><li><code>As</code>: Vector of transition matrices, one per timestep</li><li><code>bs</code>: Vector of transition offsets, one per timestep</li><li><code>Qs</code>: Vector of process noise covariances, one per timestep</li><li><code>Hs</code>: Vector of observation matrices, one per timestep</li><li><code>cs</code>: Vector of observation offsets, one per timestep</li><li><code>Rs</code>: Vector of observation noise covariances, one per timestep</li><li><code>ys</code>: Vector of observations</li></ul><p><strong>Returns</strong></p><p>Total log-likelihood: log p(y₁:T) = Σ_t log p(yₜ | y₁:ₜ₋₁)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L120-L143">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.log_likelihoods-Tuple{DiscreteLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.log_likelihoods-Tuple{DiscreteLikelihood}"><code>GeneralisedFilters.log_likelihoods</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">log_likelihoods(state::DiscreteLikelihood)</code></pre><p>Extract the log backward likelihoods from a DiscreteLikelihood.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L240-L244">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.marginalise!-Tuple{GeneralisedFilters.ParticleDistribution, Any}"><a class="docstring-binding" href="#GeneralisedFilters.marginalise!-Tuple{GeneralisedFilters.ParticleDistribution, Any}"><code>GeneralisedFilters.marginalise!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">marginalise!(state::ParticleDistribution)</code></pre><p>Compute the log-likelihood increment and normalize particle weights. This function:</p><ol><li>Computes LSE of current (post-observation) log-weights</li><li>Calculates ll<em>increment = LSE</em>after - ll_baseline</li><li>Normalizes weights by subtracting LSE_after</li><li>Resets ll_baseline to 0.0</li></ol><p>The ll<em>baseline field handles both standard particle filter and auxiliary particle filter cases through a single-scalar caching mechanism. For standard PF, ll</em>baseline equals the LSE before adding observation weights. For APF with resampling, it includes first-stage correction terms computed during the APF resampling step.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L142-L155">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.natural_params-Tuple{InformationLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.natural_params-Tuple{InformationLikelihood}"><code>GeneralisedFilters.natural_params</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">natural_params(state::InformationLikelihood)</code></pre><p>Extract the natural parameters (λ, Ω) from an InformationLikelihood.</p><p>Returns a tuple <code>(λ, Ω)</code> where λ is the information vector and Ω is the information/precision matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/containers.jl#L209-L216">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.predict"><a class="docstring-binding" href="#GeneralisedFilters.predict"><code>GeneralisedFilters.predict</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">predict([rng,] dyn, algo, iter, state, observation; kwargs...)</code></pre><p>Propagate the filtered distribution forward in time using the dynamics model.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator (optional)</li><li><code>dyn</code>: The latent dynamics model</li><li><code>algo</code>: The filtering algorithm</li><li><code>iter::Integer</code>: The current time step</li><li><code>state</code>: The filtered state distribution at time <code>iter-1</code></li><li><code>observation</code>: The observation (may be used by some proposals)</li></ul><p><strong>Returns</strong></p><p>The predicted state distribution at time <code>iter</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L38-L53">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{&lt;:GaussianPrior, &lt;:LinearGaussianLatentDynamics, &lt;:LinearGaussianObservationProcess}, KalmanSmoother, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{&lt;:GaussianPrior, &lt;:LinearGaussianLatentDynamics, &lt;:LinearGaussianObservationProcess}, KalmanSmoother, AbstractVector}"><code>GeneralisedFilters.smooth</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">smooth([rng,] model, algo, observations; t_smooth=1, callback=nothing, kwargs...)</code></pre><p>Run a forward-backward smoothing pass to compute the smoothed distribution at time <code>t_smooth</code>.</p><p>This function first runs a forward filtering pass using the Kalman filter, caching all filtered distributions, then performs backward smoothing using the Rauch-Tung-Striebel equations from time T back to <code>t_smooth</code>.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator (optional, defaults to <code>default_rng()</code>)</li><li><code>model</code>: A linear Gaussian state space model</li><li><code>algo</code>: The smoothing algorithm (e.g., <code>KalmanSmoother()</code>)</li><li><code>observations</code>: Vector of observations y₁, ..., yₜ</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>t_smooth=1</code>: The time step at which to return the smoothed distribution</li><li><code>callback=nothing</code>: Optional callback for the forward filtering pass</li></ul><p><strong>Returns</strong></p><p>A tuple <code>(smoothed, log_likelihood)</code> where:</p><ul><li><code>smoothed</code>: The smoothed distribution p(xₜ | y₁:ₜ) at time <code>t_smooth</code></li><li><code>log_likelihood</code>: The total log-likelihood from the forward pass</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">model = create_homogeneous_linear_gaussian_model(μ0, Σ0, A, b, Q, H, c, R)
smoothed, ll = smooth(model, KalmanSmoother(), observations)</code></pre><p>See also: <a href="#GeneralisedFilters.backward_smooth"><code>backward_smooth</code></a>, <a href="#GeneralisedFilters.filter-Tuple{Random.AbstractRNG, SSMProblems.AbstractStateSpaceModel, GeneralisedFilters.AbstractFilter, AbstractVector}"><code>filter</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman.jl#L192-L223">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{&lt;:GeneralisedFilters.DiscretePrior, &lt;:DiscreteLatentDynamics, &lt;:SSMProblems.ObservationProcess}, DiscreteSmoother, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.smooth-Tuple{Random.AbstractRNG, SSMProblems.StateSpaceModel{&lt;:GeneralisedFilters.DiscretePrior, &lt;:DiscreteLatentDynamics, &lt;:SSMProblems.ObservationProcess}, DiscreteSmoother, AbstractVector}"><code>GeneralisedFilters.smooth</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">smooth(rng, model::DiscreteStateSpaceModel, algo::DiscreteSmoother, observations; t_smooth=1, kwargs...)</code></pre><p>Run forward-backward smoothing for discrete state space models.</p><p>Returns the smoothed distribution at time <code>t_smooth</code> and the log-likelihood.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L184-L190">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.srkf_predict-Tuple{Distributions.MvNormal, Any}"><a class="docstring-binding" href="#GeneralisedFilters.srkf_predict-Tuple{Distributions.MvNormal, Any}"><code>GeneralisedFilters.srkf_predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">srkf_predict(state, dyn_params)</code></pre><p>Perform the square-root Kalman filter predict step.</p><p>Given the filtered state and dynamics parameters <code>(A, b, Q)</code>, compute the predicted state using QR factorization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/srkf.jl#L85-L92">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.srkf_update-Tuple{Distributions.MvNormal, Any, Any, Any}"><a class="docstring-binding" href="#GeneralisedFilters.srkf_update-Tuple{Distributions.MvNormal, Any, Any, Any}"><code>GeneralisedFilters.srkf_update</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">srkf_update(state, obs_params, observation, jitter)</code></pre><p>Perform the square-root Kalman filter update step.</p><p>Given the predicted state, observation parameters <code>(H, c, R)</code>, and observation <code>y</code>, compute the filtered state and log-likelihood using QR factorization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/srkf.jl#L112-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.step"><a class="docstring-binding" href="#GeneralisedFilters.step"><code>GeneralisedFilters.step</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">step([rng,] model, algo, iter, state, observation; kwargs...)</code></pre><p>Perform a combined predict and update step of the filtering algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator (optional)</li><li><code>model</code>: The state space model</li><li><code>algo</code>: The filtering algorithm</li><li><code>iter::Integer</code>: The current time step</li><li><code>state</code>: The current state distribution</li><li><code>observation</code>: The observation at time <code>iter</code></li></ul><p><strong>Returns</strong></p><p>A tuple <code>(new_state, log_likelihood_increment)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L20-L35">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.trajectory_logdensity-Tuple{HierarchicalSSM, GeneralisedFilters.AbstractFilter, Any, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.trajectory_logdensity-Tuple{HierarchicalSSM, GeneralisedFilters.AbstractFilter, Any, AbstractVector}"><code>GeneralisedFilters.trajectory_logdensity</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">trajectory_logdensity(model::HierarchicalSSM, af::AbstractFilter, outer_trajectory, observations)</code></pre><p>Compute the joint log-density of an outer trajectory and observations under a hierarchical SSM:</p><pre><code class="language-julia hljs">log p(u₀) + Σ_t log p(uₜ | uₜ₋₁) + log p(y₁:T | u₀:T)</code></pre><p>The last term is the marginal log-likelihood of the inner model conditioned on the outer trajectory, computed by <code>inner_loglikelihood</code> using the analytical filter <code>af</code>.</p><p>The <code>outer_trajectory</code> should be an OffsetVector indexed from 0.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L30-L41">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.trajectory_logdensity-Tuple{SSMProblems.StateSpaceModel, Any, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.trajectory_logdensity-Tuple{SSMProblems.StateSpaceModel, Any, AbstractVector}"><code>GeneralisedFilters.trajectory_logdensity</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">trajectory_logdensity(model::StateSpaceModel, trajectory, observations)</code></pre><p>Compute the joint log-density of a trajectory and observations under a regular SSM:</p><pre><code class="language-julia hljs">log p(x₀) + Σ_t [log p(xₜ | xₜ₋₁) + log p(yₜ | xₜ)]</code></pre><p>The <code>trajectory</code> should be an OffsetVector indexed from 0 (matching the prior at time 0).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/integrations/logdensity.jl#L9-L17">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.two_filter_smooth"><a class="docstring-binding" href="#GeneralisedFilters.two_filter_smooth"><code>GeneralisedFilters.two_filter_smooth</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">two_filter_smooth(filtered, backward_lik)</code></pre><p>Combine a forward filtered distribution with a backward predictive likelihood to obtain the smoothed distribution at a given time step.</p><p>The smoothed distribution is the (normalized) product:</p><p class="math-container">\[p(x_t | y_{1:T}) \propto p(x_t | y_{1:t}) \times p(y_{t+1:T} | x_t)\]</p><p>where:</p><ul><li><code>filtered</code> represents the forward filtered distribution <span>$p(x_t | y_{1:t})$</span></li><li><code>backward_lik</code> represents the backward predictive likelihood <span>$p(y_{t+1:T} | x_t)$</span></li></ul><p>Note that <code>backward_lik</code> is a likelihood (function of x), not a distribution over x.</p><p><strong>Arguments</strong></p><ul><li><code>filtered</code>: The filtered distribution <span>$p(x_t | y_{1:t})$</span></li><li><code>backward_lik</code>: The backward predictive likelihood <span>$p(y_{t+1:T} | x_t)$</span></li></ul><p><strong>Returns</strong></p><p>The smoothed distribution <span>$p(x_t | y_{1:T})$</span>.</p><p><strong>Implementations</strong></p><ul><li><strong>Linear Gaussian</strong> (<code>MvNormal</code>, <code>InformationLikelihood</code>): Combines using the product of Gaussians formula in information form.</li></ul><p><strong>Relation to <code>compute_marginal_predictive_likelihood</code></strong></p><p>Both functions take the same inputs but compute different quantities:</p><ul><li><code>two_filter_smooth</code> returns the <strong>distribution</strong> <span>$p(x_t | y_{1:T})$</span></li><li><code>compute_marginal_predictive_likelihood</code> returns the <strong>scalar</strong> <span>$p(y_{t+1:T} | y_{1:t})$</span></li></ul><p>See also: <a href="#GeneralisedFilters.backward_smooth"><code>backward_smooth</code></a>, <a href="#GeneralisedFilters.compute_marginal_predictive_likelihood-Tuple{AbstractVector, DiscreteLikelihood}"><code>compute_marginal_predictive_likelihood</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L121-L156">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.two_filter_smooth-Tuple{AbstractVector, DiscreteLikelihood}"><a class="docstring-binding" href="#GeneralisedFilters.two_filter_smooth-Tuple{AbstractVector, DiscreteLikelihood}"><code>GeneralisedFilters.two_filter_smooth</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">two_filter_smooth(filtered::AbstractVector, backward_lik::DiscreteLikelihood)</code></pre><p>Combine forward filtered distribution with backward likelihood to get smoothed distribution.</p><p>Returns the normalized smoothed distribution γ<em>t(i) ∝ π</em>t(i) * β_t(i).</p><p>All computations are performed in log-space for numerical stability.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/forward.jl#L233-L241">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.update"><a class="docstring-binding" href="#GeneralisedFilters.update"><code>GeneralisedFilters.update</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">update(obs, algo, iter, state, observation; kwargs...)</code></pre><p>Update the predicted distribution given an observation.</p><p><strong>Arguments</strong></p><ul><li><code>obs</code>: The observation process model</li><li><code>algo</code>: The filtering algorithm</li><li><code>iter::Integer</code>: The current time step</li><li><code>state</code>: The predicted state distribution</li><li><code>observation</code>: The observation at time <code>iter</code></li></ul><p><strong>Returns</strong></p><p>A tuple <code>(filtered_state, log_likelihood_increment)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/interface.jl#L56-L70">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.update_with_cache-Tuple{LinearGaussianObservationProcess, KalmanFilter, Integer, Distributions.MvNormal, AbstractVector}"><a class="docstring-binding" href="#GeneralisedFilters.update_with_cache-Tuple{LinearGaussianObservationProcess, KalmanFilter, Integer, Distributions.MvNormal, AbstractVector}"><code>GeneralisedFilters.update_with_cache</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">update_with_cache(obs, algo, iter, state, observation; kwargs...)</code></pre><p>Perform Kalman update and return cache for gradient computation.</p><p>This extends the standard update step to also return a <code>KalmanGradientCache</code> containing intermediate values needed for efficient backward gradient propagation.</p><p><strong>Returns</strong></p><p>A tuple <code>(filtered_state, log_likelihood, cache)</code> where:</p><ul><li><code>filtered_state</code>: The posterior state as <code>MvNormal</code></li><li><code>log_likelihood</code>: The log-likelihood increment</li><li><code>cache</code>: A <code>KalmanGradientCache</code> for use in backward gradient computation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/algorithms/kalman_gradient.jl#L34-L47">source</a></section></details></article><article><details class="docstring" open="true"><summary id="GeneralisedFilters.will_resample"><a class="docstring-binding" href="#GeneralisedFilters.will_resample"><code>GeneralisedFilters.will_resample</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">will_resample(resampler::AbstractResampler, state::ParticleDistribution)</code></pre><p>Determine whether a resampler will trigger resampling given the current particle state. For uncondition resamplers, always returns <code>true</code>. For conditional resamplers (e.g., <code>ESSResampler</code>), checks the resampling condition.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/TuringLang/SSMProblems.jl/blob/9b0d3bab911fa485b745fedf22b7f349990cc31c/GeneralisedFilters/src/resamplers.jl#L11-L17">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="examples/trend-inflation/">Trend Inflation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 19 February 2026 09:21">Thursday 19 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
